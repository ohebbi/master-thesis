\chapter{Optimalization of machine learning models}

This chapter is named optimalization due to its contents; here we will account for the choices we compose to optimize the four machine learning algorithms for each of the three approaches outlined in \autoref{sec:data mining}. Initially, that involves finding what information is stored within the databases and the compromise of gathering the information, which further evolves into finding optimal hyperparameters for each approach.

\begin{comment}
\section{Time of extraction and featurization}

The initial thought behind

\begin{table}[!ht]
\centering
\caption{}
\label{tab:timing-extraction}
\noindent\makebox[\textwidth]{
\begin{tabular}{M{3.0cm} M{4.0cm} M{4.0cm}}
  \hline
  \hline
  Database & Extraction period & Estimated time usage  \\
  \hline
  Materials Project & December $2020$ & $5$ min \\
  Citrine Informatics & December $2020$ & $2$ min  \\
  OQMD & December $2020$ & $3$ min \\
  AFLOW & January $2020$ - February $2021$ & $17$ days \\
  AFLOW-ML & January $2020$ - February $2021$ & $16$ days \\
  JARVIS-DFT & January $2020$ & $5$ min \\
  \hline
  \hline
\end{tabular}
}
\end{table}

\end{comment}
% 139.367, icsd: 52.116, bg: 68141
The first step of this work was to find data from Materials Project, involving entries that are associated with an ICSD structure and have a PBE-GGA calculated band gap of minimum $0.1$eV. Out of $126.335$ existing entries in Materials Project, $48.644$ ($39\%$) were found to have an associated ICSD-structure, while $65.783$ ($52\%$) materials had a calculated band gap of at least $0.1$eV. It was found that $25271$ ($20\%$) materials have the band gap minimum and an associated ICSD-structure. It should be noted that these numbers are based on data extraction in December of $2020$, while the extraction from other databases and featurization related to this work was done in the time period of December $2020$ to March $2021$. In February of $2021$, over $30.000$ new materials were added in a large update\footnote{https://matsci.org/t/materials-project-database-release-log/1609/16 23.04.2021}. These new entries are not included in this work, and therefore the number of entries in our Materials Project are based on the latest release in $2020$, which is named V2020.09.08.%, however there are present more than than $2000$ new entries that satisfy the initial MP requirement now.

Two visualizations of two different distributions of the data is found in \autoref{fig:hist_ox} and \autoref{fig:hist_bg}. The first figure visualize the distribution of oxid types as a function of compound type, and reveal that the majority of compounds are either binary, ternary, quaternary or quinary, hence the majority of the materials are oxide. This is important to know considering our labelling approaches, in particular the insightful approach where we handpicked good entries. Only a single oxid (ZnO) was deemed a potentially good candidate, which will be interesting to compare towards the different models and approaches.

\clearpage

\begin{figure}
      \centering
      \includegraphics{../predicting-solid-state-qubit-candidates/reports/figures/buildingFeatures/histogram_oxid_nelements.pdf}
      \vspace*{-130mm}
      \caption{Distribution of oxid types as a function of number of elements in compounds in the data. The majority of the entries are found as oxides, while the second most frequent type is not an oxid. }
      \label{fig:hist_ox}
\end{figure}

\begin{figure}
      \centering
      \includegraphics{../predicting-solid-state-qubit-candidates/reports/figures/buildingFeatures/histogram_bg_nelements.pdf}
      \vspace*{-130mm}
      \caption{Distribution of band gaps as function of compound type in the data. The majority of compounds are ternary and quaternary, while the simpler compounds are few.}
      \label{fig:hist_bg}
\end{figure}

\clearpage

\noindent The second figure visualize the compound type as function of band gap, as calculated by Materials Project. Most of the materials existing in the data has a band gap lower than $2.3$eV, where ternary compounds are most prominent. For larger values, we observe that quaternary compounds becomes dominant for larger values.

\section{Comparing functionals for band gaps}

Since the true size of a band gap can not be accurately determined by ab-initio calculations, we provide information regarding five different methods to obtain band gaps as visualized in \autoref{fig:band gaps}. We have extracted experimental band gaps from Citrine Informatics that match the entries made by the initial MP query, involving entries that are associated with an ICSD structure that have a PBE-GGA calculated band gaps of minimum $0.1$. All the band gaps to the left are found common with all databases through screening of correct structure, space group and ICSD-ID, while the figures to the right are only compared to the experimental database of Citrine Informatics. We found it helpful with the ICSD-tag to find similarities due to databases often have different norms and data-structures of descriptors, which proves challenging for comparison of stored calculations. If we were to exclude ICSD-tags, it would result in a much larger foundation to find similar entries, however, we found that the determination of similar entries would experience a large deviation when it comes to structures. By including an ICSD-tag, we reduce the basis of comparison but find more than $98\%$ identical space group for entries in each database compared to Materials Project.

It was found that a very small portion of the data extracted from AFLOW was associated with an ICSD-tag, only $5$ similar entries to the other databases, and therefore we have excluded the database from further consideration.

In the figures of \autoref{fig:band gaps}, we observe each entry marked as black or blue dots. The dotted lines visualize the optimal ratio of estimated band gap to experimental values, while the red lines shows an linear least square fit to the data with the scrabbled area being the $95\%$ confidence interval. The data that constitute the left figures are based on $82$ similar entries, while the right figures constitute of more entries depending on the respective database. The data restriction was due to a small experimental database.

Initally, we wanted to include the right figures in the attempt of reducing the confidence interval with increasing the data points, but instead we find that the uncertainty of the confidence interval increase for all ab-initio calculations. This is due to the fact that the majority of the new entries are found for low band gap values, where the mismatch between experimental and calculated values are the largest. The discrepency seems to be largest for values under $5$eV, where entries are either calculated to have a very large band gap where the experimental values report a very low band gap, which is also true the opposite case. One reason for this is that we have no information regarding the experiment where the band gap was determined. The information we from the experimental database is only considered the chemical formula of a compound, whereas the structure or ICSD-entry remains unknown. However, the same data of experimental values have been considered through other articles \cite{Ward2018, Ferrenti2020}.

Therefore, we find that the functional applied for Materials Project are found to underestimate the band gap with $30-60\%$ while OQMD underestimates the band gap by $25-55\%$. AFLOW-ML also severily underestimates the band gap by $30-60\%$, but additionally have problems to accurately predict if a material is a metal or not. Many materials with both experimental and ab-initio calculations that showed a band gap of more than $1$eV was predicted as metals by AFLOW-ML. JARVIS-DFT, on the other hand, was found to underestimate the band gap by $20-60\%$ for the OptB88 and $0-30\%$ for TB-mBJ functionals.



%Similar to the results of \citeauthor{Ferrenti2020} \cite{Ferrenti2020}, we find

\clearpage
\begin{figure}[ht!]
    \centering
    \begin{subfigure}[t]{1\textwidth}
        \centering
        \input{../predicting-solid-state-qubit-candidates/reports/figures/bandgaps/mp.tex}
        \caption{}
    \end{subfigure}%

    \begin{subfigure}[t]{1\textwidth}
        \centering
        \input{../predicting-solid-state-qubit-candidates/reports/figures/bandgaps/oqmd.tex}
        \caption{}
    \end{subfigure}

    \begin{subfigure}[t]{1\textwidth}
        \centering
        \input{../predicting-solid-state-qubit-candidates/reports/figures/bandgaps/aflowml.tex}
        \caption{}
    \end{subfigure}
\end{figure}

\begin{figure}[t!]\ContinuedFloat
    \centering
    \begin{subfigure}[t]{1\textwidth}
        \centering
        \input{../predicting-solid-state-qubit-candidates/reports/figures/bandgaps/jarvis_tbmbj.tex}
        \caption{}
    \end{subfigure}%

    \begin{subfigure}[t]{1\textwidth}
        \centering
        \input{../predicting-solid-state-qubit-candidates/reports/figures/bandgaps/jarvis_opt.tex}
        \caption{}
    \end{subfigure}
    \vspace*{-130mm}
    \caption{Comparison of reported experimental band gaps to those calculated by (a) Materials Project, (b) Open Quantum Materials Database, (c) AFLOW-ML, (d) JARVIS-DFT (TB-mBJ) and (e) JARVIS-DFT (OptB88). The figures to the left show reported band gaps that have been found to be common through all databases, while the figures to the right are only common with experimentally reported values from Citrine Informatics. All entries have been extracted in the period of january to march of $2021$. }
    \label{fig:band gaps}
\end{figure}

\clearpage

\section{Technical details on ML classifiers}

%In this section we will provide technical details on the classifiers considering the training process. For each approach, we will apply combinations of principal components ranging from just one to several and look at the resulting implications. For each approach we can end up with over twenty different optimalization processes, which in total could potentially result in over sixty  in total. Therefore, we will not make an extensive analysis for every model, but emphasis important distinctions between the models and provide background for principal choices made. However, it should be noted that an an extensive automated analysis is distributed through the MIT license at the Github repository \textit{predicting-solid-state-qubit-candidates} \cite{Ohebbi2021}.

In the evaluation of the approaches, we apply a $5\times 5$ stratified cross-validation when iterating through the hyperparameter combinations. We acknowledge that the three approaches experience imbalanced datasets, but from the Validation chapter we find that by adjusting the class balance helped with reducing the variance due to a very small dataset and a class ratio of $1:9$. In this section, we find all three approaches to have substantially larger datasets than the cubic perovskite dataset, thus we choose to not apply any technique for balancing the classes. Instead, we apply four different algorithms to compare them up to each other, and use four different evaluation metrics to estimate how the classifiers are performing. %Interestingly,   %There is a wide field of other techniques that deal with the vulnerabilites of imbalanced datasets \cite{Lemaitre2016}, but they a
% Thus, we choose to not apply any technique for balancing the classes, but rather find the compromise

For random forest, gradient boost and decision tree, we found that by adjusting the parameters responded to severe overfitting except for the default values defined by Scikit-learn. The only parameter that we found could potentially improve the evaluation metric $f1$ was maximum number of depth for the trees grown, which we adjusted between $1$ and $8$. For logistic regression, we choose to adjust the regulariation strength with seven logaritmical adjusted values $10^{-3}$ to $10^{5}$, and use either $200$ or $400$ iterations to reach convergence.

When searching for the optimal number of principal components, we iterated over every odd number of principal components from $1$ to the upper restricted number which defines an accumulated variance of $95\%$ from the principal component analysis. Due to the large number of principal components, we end up fitting $25$ folds for each of $1232$ parameter combinations, totalling up to $30800$ individual models, just for logistic regression for one approach. This serves as an additional motivator to keep the models simple, and accordingly shows how easy an initial complex step might evolve into an unfeasible amount of information. Therefore, we will not make an extensive analysis for every model, but emphasis important distinctions between the general models and provide background for principal choices made. However, it should be noted that a larger automated analysis is distributed through the MIT license at the Github repository \textit{predicting-solid-state-qubit-candidates} \cite{Ohebbi2021}.

\clearpage
\subsection{The Ferrenti approach}

We visualize the grid search for the optimal number of principal components in \autoref{fig:01-pca}, where we present the mean accuracy on the training set, and the balanced accuracy, precision, recall and f1 score on the test set as a function of principal components used in the models. For each principal component, we visualize the optimal combination of hyperparameters based on the f1-score in the model. Common to all models is the improvement of scores up to around $50$ principal components, where random forest and the decision tree slowly starts to overfit for larger values. For decision trees, we observe a large fluctuation for principal components larger than $100$. The f1-score is not varying as much as the other metrics due to an increasing number of positive predictions. This means that the accuracy of positive predictions are dominating the overall accuracy measurement, and we would expect a large amount of training data being predicted as positive candidates for those combinations. However, we see that the fluctuations are smaller in size for the optimal number of principal components. Similar to the decision tree is the random forest model, which also show sign of overfitting for larger values of principal components. The recall score is unaltered for increasing principal components, but consequently we find the precision declining due to a large amount of predicted false positives.  However, as a result of an ensemble of decision trees, it show smaller signs of overfitting than the indications seen by the decision tree algorithm.

Gradient boost, on the other hand, experience minor changes for larger number of principal components, where the optimal number of components marked could be $50$ principal components less without any remarks to the models metrics. We find that by using only a few principal components will make it reach almost $100\%$ training accuracy, but does not show any clear sign on overfitting. Similarly, logistic regression show signs of almost a perfect classifier, with high scores for all metrics.

In \autoref{tab:01-pc}, we find the precise measurements for each evaluation metric for the optimal number of principal components, which is visualized as dotted lines in \autoref{fig:01-pca}. The relevant hyperparameter for logistic regression were the maximum iterations, which were set at $400$, and the regulariation term, which was found optimal at $0.46$, and max iterations at $400$. For random forest and decision trees, we find the maximum depth of $7$, while gradient boost was found to overfit for deeper depths, as visualized in \autoref{fig:gb-01-overfit} and thus we found an optimal compromise at $4$. We find that the best performing model is logistic regression, but is dependent on a large amount of principal components. Random forest and gradient boost perform comparably, with and f1 score of $0.93$ and $0.95$, respectively. However, it seems that only logistic regression is able to improve for additional principal components after the first $100$.

\begin{figure}[ht!]
  \begin{subfigure}[b]{1.0\textwidth}
    \centering
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-insightful-approach-176-label.tex}
  \end{subfigure}
  \par\bigskip
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/01-ferrenti-approach-176-LOG.tex}
    \caption{}
    \label{fig:q1-LOG}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/01-ferrenti-approach-176-DT.tex}
    \caption{}
    \label{fig:q1-DT}
  \end{subfigure}

  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/01-ferrenti-approach-176-RF.tex}
    \caption{}
    \label{fig:q1-RF}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/01-ferrenti-approach-176-GB.tex}
    \caption{}
    \label{fig:q1-GB}
  \end{subfigure}
  \vspace*{-130mm}
  \caption{{Four figures displaying hyperparameter search for the Ferrenti approach. The best estimator is visualized for all hyperparameters as a function of principal components during a grid search with a $5\times5$ stratified cross validation, and the dotted lines marks the optimal hyperparameter-combination. Train stands for normal training accuracy, while test is the balanced accuracy on the test set. Precision, recall and f1 scores are based on the test set. The number of principal components that explain the $95\%$ accumulated variance is $144$, while the optimal model is found using the $f1$ score.}}
  \label{fig:01-pca}
\end{figure}
%The specific scores for the arbitrary number of principal components is found in the Appendix \autoref{appendix:Optimalization}.  The lower plots visualizes the explained variance ratio, both accumulated and stepwise.

\begin{table}[!ht]
\centering
\caption{A table of the optimal number of principal components and the respective scores (standard deviation) for the ferrenti approach, as visualized in the dash-dotted line in \autoref{fig:01-pca}.}
\label{tab:01-pc}
\noindent\makebox[\textwidth]{
\begin{tabular}{M{1.0cm} M{1.0cm} M{2.0cm} M{2.0cm}M{2.0cm}M{2.0cm} }
  \hline
  \hline
   Model & PC & Mean test &  Mean precision & Mean recall & mean f1\\
  \hline
  LOG & $171$ & $0.98(0.012)$ & $0.98(0.011)$ & $0.99(0.007)$ & $0.99(0.007)$ \\
  DT & $37$   & $0.77(0.034)$ & $0.84(0.034)$ & $0.85(0.044)$ & $0.84(0.022)$ \\
  RF & $53$   & $0.87(0.027)$ & $0.88(0.022)$ & $0.98(0.010)$ & $0.93(0.014)$ \\
  GB & $107$  & $0.92(0.016)$ & $0.92(0.015)$ & $0.98(0.010)$ & $0.95(0.009)$ \\
  \hline
\end{tabular}
}
\end{table}

In \autoref{fig:01-fi}, we visualize how the models interpret the principal components that are sorted in descending order by the explained variance, found through a $5/times 5$ stratified cross validation. We find that we need to involve $144$ principal components to reach the $95\%$ accumulated explained variance. We have visualized the first $25$ since this captures the most important information, and we note that most of the important features are within the first five principal components.

\begin{wrapfigure}{R}{0.5\textwidth}

  \begin{subfigure}[b]{1.0\textwidth}
  \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-insightful-approach-176-label.tex}
  \end{subfigure}

  \begin{subfigure}[b]{1.0\textwidth}
  \input{../predicting-solid-state-qubit-candidates/reports/figures/grid-scores/01-ferrenti-approach-GB.tex}
  \end{subfigure}
  \vspace*{-130mm}
  \caption{Parameter search for the Ferrenti approach regarding maximum depth for gradient boost for several metrics, where the error bars visualize the standard deviation.}
  \label{fig:gb-01-overfit}
\end{wrapfigure}

For logistic regression, we have visualized the mean fitted coefficients and the standard variation in \autoref{fig:01-fi-a}. Large positive or negative coefficients can be considered increasingly important, where positive (negative) coefficients will contribute to make positive (negative) predictions. The three next figures, namely the decision tree, random forest and gradient boost we find the mean impurity based feature importance, along with the standard deviation. We observe that the single most important feature for all models is the fifth principal component. Interestingly, if we select the highest values in this eigenvector, we find that the corresponding features originates from the DFT bandgap of elemental solid among elements in the composition as calculated by OQMD.

\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \input{../predicting-solid-state-qubit-candidates/reports/figures/feature-importance/01-ferrenti-approachLOG-final.tex}
    \caption{{}}
    \label{fig:01-fi-a}
  \end{subfigure}%

  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \input{../predicting-solid-state-qubit-candidates/reports/figures/feature-importance/01-ferrenti-approachDT-final.tex}
    \caption{{}}
    \label{fig:01-fi-b}
  \end{subfigure}%

  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \input{../predicting-solid-state-qubit-candidates/reports/figures/feature-importance/01-ferrenti-approachRF-final.tex}
    \caption{{}}
    \label{fig:01-fi-c}
  \end{subfigure}%

  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \input{../predicting-solid-state-qubit-candidates/reports/figures/feature-importance/01-ferrenti-approachGB-final.tex}
    \caption{{}}
    \label{fig:01-fi-d}
  \end{subfigure}%

  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \input{../predicting-solid-state-qubit-candidates/reports/figures/feature-importance/01-ferrenti-approachPC-final.tex}
    \caption{{}}
    \label{fig:01-fi-e}
  \end{subfigure}%

  \vspace*{-130mm}
  \caption{Five figures visualizing different parameters for the (e) $25$ most principal components ranked in descending order by the explained variance for the Ferrenti approach. The parameters are (a) the logistic regression coefficients, (b) decision tree feature importance, (c) random forest feature importance, (d) gradient boost feature importance and (e) explained variance that is retained by choosing each of the eigenvectors.}
  \label{fig:01-fi}
\end{wrapfigure}

After the first ten principal components, we see that the models adapt the other principal components with varying degree. Logistic regressions's coefficients experience large fluctuations, but the three remaining models find the first and second principal component important. In order of importance, we observe that the second component's largest values corresponds to the electronegativity, ionic property and covalence radius among the elements in the composition. The aggregations are either calculated as minimum, mean, standard deviation or maximum. While the first principal component has by far the largest explained variance, it does not provide any specific information of which features it represent. We observe that a variety of features is represented, such as the rows that a composition in the periodic table represents, structural packing efficiency and atomic weights of the components, but we are unable to confirm the prominent features due to small variations.

We note that looking at feature importance can be regarded as misleading for data involving correlated features, but we consider the analysis safe due to the projection of the original data to orthogonal vectors, known as principal components, which results in uncorrelated features.

\clearpage

\subsection{The augmented Ferrenti approach}

For the augmented Ferrenti approach, we find the parameter grid search for principal components visualized in \autoref{fig:02-pca}. All models experience an almost perfect recall score for $1$ principal component due to the largely imbalanced dataset with $2141$ good and $684$ bad candidates, which is a ratio of $75:25 \%$. This is a result due to the models being able to correctly label many good candidates compared to the amount of labelling them as bad candidates. On the other hand, we find a small precision for $1$ component since the model predicts many materials, both actually labelled good and bad, as good candidates, and the latter case is in particularly large. This trend is revealed when looking at the balanced accuracy score. For all figures, it remains the lowest score of the evaluation metrics largely due to the inaccuracy of true negatives for the cross validations. Therefore, one can argue that we should use the balanced accuracy score for evaluation and not the f1 score, but the choice is independent on evaluation metric since the optimal f1 score is also the optimal balanced accuracy score for all figures.

Overall, the search for optimal hyperparameters in \autoref{fig:02-pca} for the augmented Ferrenti approach bear resemblance to the \autoref{fig:01-pca} for the Ferrenti approach. Logistic regression performs optimally for many principal component, and is the only model that continues to improve with an increasing number of components. The decision trees model experience a large fluctuation of scores, where the number of false positives is dominating the balanced accuracy score. Random forest experience less fluctuations compared to the decision tree as a consequence of the ensemble decision trees, while gradient boost does not improve after around $100$ principal components.
\begin{table}[!ht]
\centering
\caption{A table of the optimal number of principal components and the respective scores (standard deviation), as visualized in the dash-dotted line in \autoref{fig:02-pca}.}
\label{tab:02-pc}
\noindent\makebox[\textwidth]{
\begin{tabular}{M{1.0cm} M{1.0cm} M{2.0cm} M{2.0cm}M{2.0cm}M{2.0cm} }
  \hline
  \hline
   Model & PC & Mean test &  Mean precision & Mean recall & mean f1\\
  \hline
  LOG & $175$ & $0.98(0.008)$ & $0.99(0.004)$ & $0.99(0.004)$ & $0.99(0.003)$ \\
  DT & $25$   & $0.69(0.034)$ & $0.86(0.015)$ & $0.93(0.021)$ & $0.90(0.008)$ \\
  RF & $25$   & $0.70(0.028)$ & $0.86(0.011)$ & $1.00(0.003)$ & $0.93(0.006)$ \\
  GB & $93$   & $0.85(0.025)$ & $0.93(0.011)$ & $0.99(0.004)$ & $0.96(0.007)$ \\
  \hline
\end{tabular}
}
\end{table}

\begin{figure}[ht!]
  \begin{subfigure}[b]{1.0\textwidth}
    \centering
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-insightful-approach-176-label.tex}
  \end{subfigure}
\par\bigskip
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/02-augmented-ferrenti-approach-176-LOG.tex}
    \caption{}
    \label{fig:q2-LOG}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/02-augmented-ferrenti-approach-176-DT.tex}
    \caption{}
    \label{fig:q2-DT}
  \end{subfigure}

  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/02-augmented-ferrenti-approach-176-RF.tex}
    \caption{}
    \label{fig:q2-RF}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/02-augmented-ferrenti-approach-176-GB.tex}
    \caption{}
    \label{fig:q2-GB}
  \end{subfigure}
  \vspace*{-130mm}
  \caption{{Four figures displaying hyperparameter search for the augmented Ferrenti approach. The best estimator is visualized for all hyperparameters as a function of principal components during a grid search with a $5\times5$ stratified cross validation, and the dotted lines marks the optimal hyperparameter-combination. Train stands for normal training accuracy, while test is the balanced accuracy on the test set. Precision, recall and f1 scores are based on the test set. The number of principal components that explain the $95\%$ accumulated variance is $159$, while the optimal model is found using the $f1$ score.}}
  \label{fig:02-pca}
\end{figure}

The optimal hyperparameters are summarized in \autoref{tab:02-pc}. We find that the logistic regression model with $175$ principal components peform more or less as a perfect classifier with overall high scores. The decision tree and random forest models have similar balanced accuracy score with $0.69$ and $0.70$, respectively, due to challenges associated in predicting true negative labels for $25$ principal components. Lastly, we find gradient boost perform optimally at $93$ principal components with a balanced accuracy score of $0.85$.

\begin{wrapfigure}{R}{0.5\textwidth}

  \begin{subfigure}[b]{1.0\textwidth}
  \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-insightful-approach-176-label.tex}
  \end{subfigure}

  \begin{subfigure}[b]{1.0\textwidth}
  \input{../predicting-solid-state-qubit-candidates/reports/figures/grid-scores/02-augmented-ferrenti-approach-LOG.tex}
  \end{subfigure}
  \vspace*{-130mm}
  \caption{Parameter search for the augmented Ferrenti approach regarding regularization parameter for logistic regression for several metrics, where the error bars visualize the standard deviation.}
  \label{fig:log-02-overfit}
\end{wrapfigure}


The relevant hyperparameters of logistic regression were the regularization strength, which was set as $0.46$, as visualized in figureÂ \autoref{fig:log-02-overfit}, and we set maximum iterations at $400$. Smaller regularization values resulted in worse scores, while increasing values did not noteworthy alter the results. The decision tree and random forest found an optimal maximum depth of $7$, where smaller values resulted in low precision but high recall. Therefore, the choice was made to fasciliate a compromise between precision and recall. For gradient boost, we find the optimal maximum depth as $4$ due to a decline in overall metrics for increasing depth except for training accuracy, which could potentially result in overfitting.

%Four figures displaying hyperparameter search for the second approach. The best estimator is visualized for all hyperparameters as a function of principal components during a grid search with a 5x5 stratified cross validation. The lower plots visualizes the explained variance ratio, both accumulated and stepwise. The dotted lines marks the optimal hyperparameter-combination, while the error bars display the standard deviation.

The interpretation of feature importance of for the Ferrenti approach is substantially more difficult than in the Ferrenti approach. We find for logistic regression and decision trees that no feature is different than any other in the cross validation due to a large variation of models. However, we find that random forest and gradient boost experience the fifth principal component as important. Similar to the Ferrenti approach, the corresponding features with highest value for the first principal component originates the DFT bandgap of elemental sold among elements in the composition.

\subsection{The insightful approach}

Lastly, we turn to the insightful approach, which involves $418$ bad and $172$ good candidates in the imbalanced training set. However, in contrast to the two other datasets, the majority of the entries are labelled as bad candidates.

The grid search for the optimal number of principal components is visualized in \autoref{fig:03-pca}. Interestingly, we find that all models experience high scores for just a few principal component, where $1$ principal component earns at least $0.875$ in score for all evaluation metrics. This information was also revealed for an earlier 2D-visualization of a scatter plot showing the two most important principal components in \autoref{fig:2dscatterplotpca}, and consequently can make the models find the optimal decision boundary more easily.

Logistic regression experience improvement of all scores for increasing number of principal components, yet only up $5\%$ in scores compared to the $1D$-representation of $1$ principal components. Thus, one can argue if the increase in performance is worth it considering a one-dimensional representation with just a few percentage loss of performance. However, with multiple principal components, we find the largest increase in precision, which is a sign that the one-dimensional representation tend to wrongly predicts candidates as good when they are in fact bad. The decision tree and the random forest models exhibit best performance for just a few principal components, and experience considerable overfitting for larger values. Gradient boost, in contrast to the two other approaches, also experience best performance for a few principal components.

The optimal hyperparameters are summarized in \autoref{tab:03-pca}, where all models exhibit high evalution metrics. Importantly, we find the difference in number of principal component as most prominent, where logistic regression finds an optimum at $129$ with the f1 score of $0.94$. The decision tree model use only $3$ principal components to achieve a $f1$ score of $0.91$, while random forest needs $11$ principal components to gain a f1 score of $0.94$. Lastly, gradient boost performs optimally at $7$ principal components with a mean f1 score of $0.93$. The relevant hyperparameters was the regularization term for logistic regression, which was set as $0.021$, and the maximum number of iterations as $200$. The decision tree used an maximum depth of $4$, where larger values increased the training accuracy but not any other metric. Random forest was set with maximum depth of $7$, and gradient boost was given $4$.

\begin{figure}[ht!]
  \begin{subfigure}[b]{1.0\textwidth}
    \centering
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-insightful-approach-176-label.tex}
  \end{subfigure}
  \par\bigskip
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-insightful-approach-176-LOG.tex}
    \caption{}
    \label{fig:q3-LOG}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-insightful-approach-176-DT.tex}
    \caption{}
    \label{fig:q3-DT}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-insightful-approach-176-RF.tex}
    \caption{}
    \label{fig:q3-RF}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-insightful-approach-176-GB.tex}
    \caption{}
    \label{fig:q3-GB}
  \end{subfigure}

  \vspace*{-130mm}
  \caption{{Four figures displaying hyperparameter search for the insightful approach. The best estimator is visualized for all hyperparameters as a function of principal components during a grid search with a $5\times5$ stratified cross validation, and the dotted lines marks the optimal hyperparameter-combination. Train stands for normal training accuracy, while test is the balanced accuracy on the test set. Precision, recall and f1 scores are based on the test set. The number of principal components that explain the $95\%$ accumulated variance is $103$, while the optimal model is found using the $f1$ score.}}
  \label{fig:03-pca}
\end{figure}


\begin{table}[!ht]
\centering
\caption{A table of the optimal number of principal components and the respective scores (standard deviation), as visualized in the dash-dotted line in \autoref{fig:03-pca}.}
\label{tab:03-pca}
\noindent\makebox[\textwidth]{
\begin{tabular}{M{1.0cm} M{1.0cm} M{2.0cm} M{2.0cm}M{2.0cm}M{2.0cm} }
  \hline
  \hline
   Model & PC & Mean test & Mean precision & Mean recall  & mean f1\\
  \hline
  LOG & $129$ & $0.96(0.018)$ & $0.93(0.041)$ & $0.96(0.036)$ & $0.94(0.025)$ \\
  DT & $3$    & $0.94(0.025)$ & $0.91(0.048)$ & $0.92(0.050)$ & $0.91(0.032)$ \\
  RF & $11$   & $0.96(0.019)$ & $0.93(0.039)$ & $0.95(0.040)$ & $0.94(0.024)$ \\
  GB & $7$    & $0.95(0.023)$ & $0.92(0.044)$ & $0.94(0.047)$ & $0.93(0.030)$ \\
  \hline
\end{tabular}
}
\end{table}

\newpage

\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \input{../predicting-solid-state-qubit-candidates/reports/figures/feature-importance/03-insightful-approachLOG-final.tex}
    \caption{{}}
    \label{fig:03-fi-a}
  \end{subfigure}%

  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \input{../predicting-solid-state-qubit-candidates/reports/figures/feature-importance/03-insightful-approachDT-final.tex}
    \caption{{}}
    \label{fig:03-fi-b}
  \end{subfigure}%

  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \input{../predicting-solid-state-qubit-candidates/reports/figures/feature-importance/03-insightful-approachRF-final.tex}
    \caption{{}}
    \label{fig:03-fi-c}
  \end{subfigure}%

  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \input{../predicting-solid-state-qubit-candidates/reports/figures/feature-importance/03-insightful-approachGB-final.tex}
    \caption{{}}
    \label{fig:03-fi-d}
  \end{subfigure}%

  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \input{../predicting-solid-state-qubit-candidates/reports/figures/feature-importance/03-insightful-approachPC-final.tex}
    \caption{{}}
    \label{fig:03-fi-e}
  \end{subfigure}%

  \vspace*{-130mm}
  \caption{Five figures visualizing different parameters for the (e) $10$ most principal components ranked in descending order by the explained variance for the insightful approach. The parameters are (a) the logistic regression coefficients, (b) decision tree feature importance, (c) random forest feature importance, (d) gradient boost feature importance and (e) explained variance that is retained by including each of the eigenvectors.}
  \label{fig:03-fi}
\end{wrapfigure}

The insightful approach differs in many aspects from the Ferrenti or augmented Ferrenti approach. Firstly, we find that the number of principal components neccessary to obtain $95\%$ variance is reduced to $103$ components, which is $41$ and $56$ less than the Ferrenti or augmented Ferrenti approach, respectively. Thus, the variance of the training set is found to be described with fewer principal components, indicating a simpler model.

Secondly, we find that the first principal component is by far the most important feature for all models, as visualized in \autoref{fig:03-fi}. This is part of the reason why we experience a large accuracy for only a single feature, seen in \autoref{fig:03-pca}. The first principal component's corresponding features are challenging to explain due to small variations of values. However, it differs when it comes which top features it describes, which includes bond orientational parameters, coordination numbers and radial distribution function of a compound's crystal system.

Thirdly, the insightful approach differs in how much explained variation is retained by the first component, which is $20\%$ while it is $14\%$ for the Ferrenti approach and $11\%$ for the augmented Ferrenti approach. We find the difference striking considering the approaches share the same ultimate goal, but where the training set apparantely constitutes of large variations.

\begin{figure}[h!]
    \centering
    \includegraphics[trim={2.8cm 0cm 0cm 0cm},clip, scale=1]{../predicting-solid-state-qubit-candidates/reports/figures/pca-3d-plots/3d-training-iso.pdf}
  \vspace*{-135mm}
  \caption{A three dimensional scatter plots visualizing the labelled training data and the isosurface of decision tree's decision boundary. Limegreen indicates good candidates, while tomato corresponds to bad candidates. Isosurface represents probability.}
  \label{fig:3d-iso}
\end{figure}

Due to that the optimal decision tree model chose $3$ dimensions, we seize the occasion and visualize a scatter plot for the training data in \autoref{fig:3d-iso}. The tomato color visualize bad candidates, while limegreen corresponds to good candidates. Additionally, we have visualized an isosurface representing the algorithm's decision boundary. Due to a rather sharp transition, we have restricted the probability down to $0.05\%$ of being labelled a good candidate, and the remaining area without isosurface is considered unfit for what we are looking for. We can observe that the model easily distinguish most of the points, but not being able to capture all of the variation in the data.

Importantly, the visualization allows us to shape a picture of the mapping by the principal component analysis. There are mainly three large clusters of data points where the largest is ZnS, second largest SiC and the smallest cluster C. % where the largest constitute of all the variations of ZnS and the other of SiC.
Close to the ZnS-cluster, we find ZnSe, ZnTe, CdS and GaAs, involving both two and three-dimensional strucutures. From this, it is clear that the decision tree is not able to distinguish between two and three-dimensional structures. The SiC-cluster is mostly by itself, with the closest entries being AlN. The cluster consisting of C, however, is more spread than the two latter and is accompanied by BN. Close to the decision boundary, we find many entries of Si and GaN. On the edge of the border are some of the oxides, such as ZnO, while by crossing the boundary we find oxides such as CoO and SiO$_2$, and the ionic compound NaCl. Interestingly, we find the two-dimensional good candidates MoS$_2$, WS$_2$ and WSe$_2$ close together but far into the area of bad candidates.

During the $5\times 5$ cross validation, we find that all models except for decision trees are able to predict the true label of bad candidates over $50 \%$ of the time. The decision tree model predicts the two-dimensional materials MoS$_2$, WS$_2$ and WSe$_2$ as bad candidates consistently. Of the good candidates, we find that the decision tree model wrongly predicts the true labels of complex or nano-structures of C, GaAs, SiC, CoO and Si more than $50\%$ of the time. Random forest and gradient boost correctly predicts the true labels of all candidates more than $50\%$ of the time, while logistic regression misses out on the two-dimensional strucutres GaAs and SiC.

%from the trainthree principal components and the
