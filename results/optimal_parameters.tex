\chapter{Optimalization}

This chapter is named optimalization due to its contents; here we will account for the choices we compose to optimize a potential machine learning algorithm. Initially, that involves finding what information is stored within the databases and the compromise of gathering the information, which further evolves into finding optimal hyperparameters for each approach.

\begin{comment}
\section{Time of extraction and featurization}

The initial thought behind

\begin{table}[!ht]
\centering
\caption{}
\label{tab:timing-extraction}
\noindent\makebox[\textwidth]{
\begin{tabular}{M{3.0cm} M{4.0cm} M{4.0cm}}
  \hline
  \hline
  Database & Extraction period & Estimated time usage  \\
  \hline
  Materials Project & December $2020$ & $5$ min \\
  Citrine Informatics & December $2020$ & $2$ min  \\
  OQMD & December $2020$ & $3$ min \\
  AFLOW & January $2020$ - February $2021$ & $17$ days \\
  AFLOW-ML & January $2020$ - February $2021$ & $16$ days \\
  JARVIS-DFT & January $2020$ & $5$ min \\
  \hline
  \hline
\end{tabular}
}
\end{table}

\end{comment}
% 139.367, icsd: 52.116, bg: 68141
The first step of this work was to find data from Materials Project, involving entries that are associated with an ICSD structure and have a PBE-GGA calculated band gap of minimum $0.1$eV. Out of $126.335$ existing entries in Materials Project, $48.644$ ($39\%$) were found to have an associated ICSD-structure, while $65.783$ ($52\%$) materials had a calculated band gap of at least $0.1$eV. It was found that $25271$ ($20\%$) materials have the band gap minimum and an associated ICSD-structure. It should be noted that these numbers are based on data extraction in December of $2020$, while the extraction from other databases and featurization related to this work was done in the time period of December $2020$ to March $2021$. In February of $2021$, over $30.000$ new materials were added in a large update\footnote{https://matsci.org/t/materials-project-database-release-log/1609/16 23.04.2021}. These new entries are not included in this work, and therefore the number of entries in our Materials Project are based on the latest release in $2020$, which is named V2020.09.08.%, however there are present more than than $2000$ new entries that satisfy the initial MP requirement now.

Two visualizations of two different distributions of the data is found in figure \ref{fig:hist_ox} and \ref{fig:hist_bg}. The first figure visualize the distribution of oxid types as a function of compound type, and reveal that the majority of compounds are either binary, ternary, quaternary or quinary, where the majority of the materials are either oxide or not. This is important to know considering our labelling approaches, in particular the insightful approach where we handpicked good entries. None of the entries labelled as good candidates were oxides, which means that potentially half of the the test set could potentially be considering as bad candidates by the models.

\clearpage

\begin{figure}
      \centering
      \includegraphics{../predicting-solid-state-qubit-candidates/reports/figures/buildingFeatures/histogram_oxid_nelements.pdf}
      \vspace*{-130mm}
      \caption{Distribution of oxid types as a function of number of elements in compounds in the data. The majority of the entries are found as oxides, while the second most frequent type is not an oxid. }
      \label{fig:hist_ox}
\end{figure}

\begin{figure}
      \centering
      \includegraphics{../predicting-solid-state-qubit-candidates/reports/figures/buildingFeatures/histogram_bg_nelements.pdf}
      \vspace*{-130mm}
      \caption{Distribution of band gaps as function of compound type in the data. The majority of compounds are ternary and quaternary, while the simpler compounds are few.}
      \label{fig:hist_bg}
\end{figure}

\clearpage

\noindent The second figure visualize the compound type as function of band gap, as calculated by Materials Project. Most of the materials existing in the data has a band gap lower than $2.3$eV, where ternary compounds are most prominent. For larger values, we observe that quaternary compounds becomes dominant for larger values.

\section{Comparing functionals for band gaps}

Since the true size of a band gap can not be accurately determined by ab-initio calculations, we provide information regarding five different methods to obtain band gaps as visualized in figure \ref{fig:band gaps}. We have extracted experimental band gaps from Citrine Informatics that match the entries made by the initial MP query, involving entries that are associated with an ICSD structure that have a PBE-GGA calculated band gaps of minimum $0.1$. All the band gaps to the left are found common with all databases through screening of correct structure, space group and ICSD-ID, while the figures to the right are only compared to the experimental database of Citrine Informatics. We found it helpful with the ICSD-tag to find similarities due to databases often have different norms and data-structures of descriptors, which proves challenging for comparison of stored calculations. If we were to exclude ICSD-tags, it would result in a much larger foundation to find similar entries, however, we found that the determination of similar entries would experience a large deviation when it comes to structures. By including an ICSD-tag, we reduce the basis of comparison but find more than $98\%$ identical space group for entries in each database compared to Materials Project.

It was found that a very small portion of the data extracted from AFLOW was associated with an ICSD-tag, only $5$ similar entries to the other databases, and therefore we have excluded the database from further consideration.

In the figures of \ref{fig:band gaps}, we observe each entry marked as black or blue dots. The dotted lines visualize the optimal ratio of estimated band gap to experimental values, while the red lines shows an linear least square fit to the data with the scrabbled area being the $95\%$ confidence interval. The data that constitute the left figures are based on $82$ similar entries, while the right figures constitute of more entries depending on the respective database. The data restriction was due to a small experimental database.

Initally, we wanted to include the right figures in the attempt of reducing the confidence interval with increasing the data points, but instead we find that the uncertainty of the confidence interval increase for all ab-initio calculations. This is due to the fact that the majority of the new entries are found for low band gap values, where the mismatch between experimental and calculated values are the largest. The discrepency seems to be largest for values under $5$eV, where entries are either calculated to have a very large band gap where the experimental values report a very low band gap, which is also true the opposite case. One reason for this is that we have no information regarding the experiment where the band gap was determined. The information we from the experimental database is only considered the chemical formula of a compound, whereas the structure or ICSD-entry remains unknown. However, the same data of experimental values have been considered through other articles \cite{Ward2018, Ferrenti2020}.

Therefore, we find that the functional applied for Materials Project are found to underestimate the band gap with $30-60\%$ while OQMD underestimates the band gap by $25-55\%$. AFLOW-ML also severily underestimates the band gap by $30-60\%$, but additionally have problems to accurately predict if a material is a metal or not. Many materials with both experimental and ab-initio calculations that showed a band gap of more than $1$eV was predicted as metals by AFLOW-ML. JARVIS-DFT, on the other hand, was found to underestimate the band gap by $20-60\%$ for the OptB88 and $0-30\%$ for TB-mBJ functionals.



%Similar to the results of \citeauthor{Ferrenti2020} \cite{Ferrenti2020}, we find

\clearpage
\begin{figure}[ht!]
    \centering
    \begin{subfigure}[t]{1\textwidth}
        \centering
        \input{../predicting-solid-state-qubit-candidates/reports/figures/bandgaps/mp.tex}
        \caption{}
    \end{subfigure}%

    \begin{subfigure}[t]{1\textwidth}
        \centering
        \input{../predicting-solid-state-qubit-candidates/reports/figures/bandgaps/oqmd.tex}
        \caption{}
    \end{subfigure}

    \begin{subfigure}[t]{1\textwidth}
        \centering
        \input{../predicting-solid-state-qubit-candidates/reports/figures/bandgaps/aflowml.tex}
        \caption{}
    \end{subfigure}
\end{figure}

\begin{figure}[t!]\ContinuedFloat
    \centering
    \begin{subfigure}[t]{1\textwidth}
        \centering
        \input{../predicting-solid-state-qubit-candidates/reports/figures/bandgaps/jarvis_tbmbj.tex}
        \caption{}
    \end{subfigure}%

    \begin{subfigure}[t]{1\textwidth}
        \centering
        \input{../predicting-solid-state-qubit-candidates/reports/figures/bandgaps/jarvis_opt.tex}
        \caption{}
    \end{subfigure}
    \vspace*{-130mm}
    \caption{Comparison of reported experimental band gaps to those calculated by (a) Materials Project, (b) Open Quantum Materials Database, (c) AFLOW-ML, (d) JARVIS-DFT (TB-mBJ) and (e) JARVIS-DFT (OptB88). The figures to the left show reported band gaps that have been found to be common through all databases, while the figures to the right are only common with experimentally reported values from Citrine Informatics. All entries have been extracted in the period of january to march of $2021$. }
    \label{fig:band gaps}
\end{figure}

\clearpage

\section{Technical details on ML classifiers}

%In this section we will provide technical details on the classifiers considering the training process. For each approach, we will apply combinations of principal components ranging from just one to several and look at the resulting implications. For each approach we can end up with over twenty different optimalization processes, which in total could potentially result in over sixty  in total. Therefore, we will not make an extensive analysis for every model, but emphasis important distinctions between the models and provide background for principal choices made. However, it should be noted that an an extensive automated analysis is distributed through the MIT license at the Github repository \textit{predicting-solid-state-qubit-candidates} \cite{Ohebbi2021}.

In the evaluation of the approaches, we apply a $5\times 5$ stratified cross-validation when iterating through the hyperparameter combinations. For random forest, gradient boost and decision tree, we found that by adjusting the parameters responded to severe overfitting except for the default values defined by Scikit-learn. The only parameter that we found could potentially improve the evaluation metric $f1$ was maximum number of depth for the trees grown, which we adjusted between $1$ and $8$. For logistic regression, we choose to adjust the regulariation strength with seven logaritmical adjusted values $10^{-3}$ to $10^{5}$, and use either $200$ or $400$ iterations to reach convergence.

When searching for the optimal number of principal components, we iterated over every odd number of principal components from $1$ to the upper restricted number which defines an accumulated variance of $95\%$ from the principal component analysis. Due to the large number of principal components, we end up fitting $25$ folds for each of $1232$ parameter combinations, totalling up to $30800$ individual models, just for logistic regression for one approach. This serves as an additional motivator to keep the models simple, and accordingly shows how easy an initial complex step might evolve into an unfeasible amount of information. Therefore, we will not make an extensive analysis for every model, but emphasis important distinctions between the general models and provide background for principal choices made. However, it should be noted that a larger automated analysis is distributed through the MIT license at the Github repository \textit{predicting-solid-state-qubit-candidates} \cite{Ohebbi2021}.

\subsection{The Ferrenti approach}

We visualize the grid search for the optimal number of principal components in figure \ref{fig:01-pca}, where we present the mean accuracy on the training set, and the balanced accuracy, precision, recall and f1 score on the test set as a function of principal components used in the models. For each principal component, we visualize the optimal combination of hyperparameters based on the f1-score in the model. Common to all models is the improvement of scores up to around $50$ principal components, where random forest and the decision tree slowly starts to overfit for larger values. For decision trees, we observe a large fluctuation for principal components larger than $100$. The f1-score is not varying as much as the other metrics due to an increasing number of positive predictions. This means that the accuracy of positive predictions are dominating the overall accuracy measurement, and we would expect a large amount of training data being predicted as positive candidates for those combinations. However, we see that the fluctuations are smaller in size for the optimal number of principal components. Similar to the decision tree is the random forest model, which also show sign of overfitting for larger values of principal components. The recall score is unaltered for increasing principal components, but consequently we find the precision declining due to a large amount of predicted false positives.  However, as a result of several weak trees, it show smaller signs of overfitting than the indications seen by the decision tree algorithm.

Gradient boost, on the other hand, experience minor changes for larger number of principal components, where the optimal number of components marked could be $50$ principal components less without any remarks to the models metrics. We find that by using only a few principal components will make it reach almost $100\%$ training accuracy, but does not show any clear sign on overfitting. Similarly, logistic regression show signs of almost a perfect classifier, with high scores for all metrics.

\begin{wrapfigure}{R}{0.5\textwidth}

  \begin{subfigure}[b]{1.0\textwidth}
  \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-insightful-approach-176-label.tex}
  \end{subfigure}

  \begin{subfigure}[b]{1.0\textwidth}
  \input{../predicting-solid-state-qubit-candidates/reports/figures/grid-scores/01-ferrenti-approach-GB.tex}
  \end{subfigure}
  \vspace*{-130mm}
  \caption{Parameter search for the Ferrenti approach regarding maximum depth for gradient boost for several metrics, where the error bars visualize the standard deviation.}
  \label{fig:gb-01-overfit}
\end{wrapfigure}

In table \ref{tab:01-pc}, we find the precise measurements for each evaluation metric for the optimal number of principal components, which is visualized as dotted lines in figure \ref{fig:01-pca}. The relevant hyperparameter for logistic regression were the maximum iterations, which were set at $400$, and the regulariation term, which was found optimal at $10^{-1}$. For random forest and decision trees, we find the maximum depth of $7$, while gradient boost was found to overfit for deeper depths, as visualized in figure \ref{fig:gb-01-overfit} and thus we found an optimal compromise at $4$. We find that the best performing model is logistic regression, but is dependent on a large amount of principal components. Random forest and gradient boost perform comparably, with and f1 score of $0.93$ and $0.95$, respectively. However, it seems that only logistic regression is able to improve for additional principal components after the first $100$.

\clearpage

\begin{figure}[ht!]
  \begin{subfigure}[b]{1.0\textwidth}
    \centering
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-insightful-approach-176-label.tex}
  \end{subfigure}
  \par\bigskip
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/01-ferrenti-approach-176-LOG.tex}
    \caption{}
    \label{fig:q1-LOG}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/01-ferrenti-approach-176-DT.tex}
    \caption{}
    \label{fig:q1-DT}
  \end{subfigure}

  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/01-ferrenti-approach-176-RF.tex}
    \caption{}
    \label{fig:q1-RF}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/01-ferrenti-approach-176-GB.tex}
    \caption{}
    \label{fig:q1-GB}
  \end{subfigure}
  \vspace*{-130mm}
  \caption{{Four figures displaying hyperparameter search for the Ferrenti approach. The best estimator is visualized for all hyperparameters as a function of principal components during a grid search with a $5\times5$ stratified cross validation, and the dotted lines marks the optimal hyperparameter-combination. Train stands for normal training accuracy, while test is the balanced accuracy on the test set. Precision, recall and f1 scores are based on the test set. The upper limit of principal components is decided by the explained accumulated variance at $95\%$, while the optimal model is found using the $f1$ score.}}
  \label{fig:01-pca}
\end{figure}
%The specific scores for the arbitrary number of principal components is found in the Appendix \ref{appendix:Optimalization}.  The lower plots visualizes the explained variance ratio, both accumulated and stepwise.

\begin{table}[!ht]
\centering
\caption{A table of the optimal number of principal components and the respective scores (standard deviation), as visualized in the dash-dotted line in figure \ref{fig:01-pca}.}
\label{tab:01-pc}
\noindent\makebox[\textwidth]{
\begin{tabular}{M{1.0cm} M{1.0cm} M{2.0cm} M{2.0cm}M{2.0cm}M{2.0cm} }
  \hline
  \hline
   Model & PC & Mean test &  Mean precision & Mean recall & mean f1\\
  \hline
  LOG & $171$ & $0.98(0.012)$ & $0.98(0.011)$ & $0.99(0.007)$ & $0.99(0.007)$ \\
  DT & $37$   & $0.77(0.034)$ & $0.84(0.034)$ & $0.85(0.044)$ & $0.84(0.022)$ \\
  RF & $53$   & $0.87(0.027)$ & $0.88(0.022)$ & $0.98(0.010)$ & $0.93(0.014)$ \\
  GB & $107$  & $0.92(0.016)$ & $0.92(0.015)$ & $0.98(0.010)$ & $0.95(0.009)$ \\
  \hline
\end{tabular}
}
\end{table}

\begin{comment}
\begin{figure}[!tbp]
  \begin{subfigure}[b]{0.5\textwidth}
    \include{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/01-ferrenti-approach-5-RF.pgf}
    \caption{}
    \label{fig:q1-GB}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \include{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/01-ferrenti-approach-5-GB.pgf}
    \caption{}
    \label{fig:q1-RF}
  \end{subfigure}

  \begin{subfigure}[b]{0.5\textwidth}
    \include{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/01-ferrenti-approach-5-DT.pgf}
    \caption{}
    \label{fig:q1-DT}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \include{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/01-ferrenti-approach-5-LOG.pgf}
    \caption{}
    \label{fig:q1-LOG}
  \end{subfigure}
  \vspace*{-95mm}
  \caption{{Four figures displaying hyperparameter search for the first approach. The best estimator is visualized for all hyperparameters as a function of principal components during a grid search with a 5x5 stratified cross validation. The lower plots visualizes the explained variance ratio, both accumulated and stepwise. The dotted lines marks the optimal hyperparameter-combination, while the error bars display the standard deviation. }}
\end{figure}
\end{comment}



\subsection{The augmented Ferrenti approach}

For the augmented Ferrenti approach, we find the parameter grid search for principal components visualized in figure \ref{fig:02-pca}. All models experience an almost perfect recall score for $1$ principal component due to the largely imbalanced dataset with $2141$ good and $684$ bad candidates, which is a ratio of $75:25 \%$. This is a result due to the models being able to correctly label many good candidates compared to the amount of labelling them as bad candidates. On the other hand, we find a small precision for $1$ component since the model predicts many materials, both actually labelled good and bad, as good candidates, and the latter case is in particularly large. This trend is revealed when looking at the balanced accuracy score. For all figures, it remains the lowest score of the evaluation metrics largely due to the inaccuracy of true negatives for the cross validations. Therefore, one can argue that we should use the balanced accuracy score for evaluation and not the f1 score, but the choice is independent on evaluation metric since the optimal f1 score is also the optimal balanced accuracy score for all figures.

Overall, the search for optimal hyperparameters in figure \ref{fig:02-pca} for the augmented Ferrenti approach bear resemblance to the figure \ref{fig:01-pca} for the Ferrenti approach. Logistic regression performs optimally for many principal component, and is the only model that continues to improve with an increasing number of components. The decision trees model experience a large fluctuation of scores, where the number of false positives is dominating the balanced accuracy score. Random forest experience less fluctuations compared to the decision tree as a consequence of the many weak learners, while gradient boost does not improve after around $100$ principal components.

The optimal hyperparameters are summarized in table \ref{tab:02-pc}. We find that the logistic regression model with $175$ principal components peform more or less as a perfect classifier with overall high scores. The decision tree and random forest models have similar balanced accuracy score with $0.69$ and $0.70$, respectively, due to challenges associated in predicting true negative labels for $25$ principal components. Lastly, we find gradient boost perform optimally at $93$ principal components with a balanced accuracy score of $0.85$. The relevant hyperparameters were the regularization strength of logistic regression, which was set as $10^{-1}$. Smaller values resulted in worse scores, while increasing values did not noteworthy alter the results. The decision tree and random forest found an optimal maximum depth of $7$, where smaller values resulted in low precision but high recall. Therefore, the choice was made to fasciliate a compromise between precision and recall. For gradient boost, we find the optimal maximum depth as $4$ due to a decline in overall metrics for increasing depth except for training accuracy, which could potentially result in overfitting.

\begin{table}[!ht]
\centering
\caption{A table of the optimal number of principal components and the respective scores (standard deviation), as visualized in the dash-dotted line in figure \ref{fig:02-pca}.}
\label{tab:02-pc}
\noindent\makebox[\textwidth]{
\begin{tabular}{M{1.0cm} M{1.0cm} M{2.0cm} M{2.0cm}M{2.0cm}M{2.0cm} }
  \hline
  \hline
   Model & PC & Mean test &  Mean precision & Mean recall & mean f1\\
  \hline
  LOG & $175$ & $0.98(0.008)$ & $0.99(0.004)$ & $0.99(0.004)$ & $0.99(0.003)$ \\
  DT & $25$   & $0.69(0.034)$ & $0.86(0.015)$ & $0.93(0.021)$ & $0.90(0.008)$ \\
  RF & $25$   & $0.70(0.028)$ & $0.86(0.011)$ & $1.00(0.003)$ & $0.93(0.006)$ \\
  GB & $93$   & $0.85(0.025)$ & $0.93(0.011)$ & $0.99(0.004)$ & $0.96(0.007)$ \\
  \hline
\end{tabular}
}
\end{table}

\clearpage
\begin{figure}[ht!]
  \begin{subfigure}[b]{1.0\textwidth}
    \centering
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-insightful-approach-176-label.tex}
  \end{subfigure}
\par\bigskip
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/02-augmented-ferrenti-approach-176-LOG.tex}
    \caption{}
    \label{fig:q2-LOG}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/02-augmented-ferrenti-approach-176-DT.tex}
    \caption{}
    \label{fig:q2-DT}
  \end{subfigure}

  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/02-augmented-ferrenti-approach-176-RF.tex}
    \caption{}
    \label{fig:q2-RF}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/02-augmented-ferrenti-approach-176-GB.tex}
    \caption{}
    \label{fig:q2-GB}
  \end{subfigure}
  \vspace*{-130mm}
  \caption{{Four figures displaying hyperparameter search for the augmented Ferrenti approach. The best estimator is visualized for all hyperparameters as a function of principal components during a grid search with a $5\times5$ stratified cross validation, and the dotted lines marks the optimal hyperparameter-combination. Train stands for normal training accuracy, while test is the balanced accuracy on the test set. Precision, recall and f1 scores are based on the test set. The upper limit of principal components is decided by the explained accumulated variance at $95\%$, while the optimal model is found using the $f1$ score.}}
  \label{fig:02-pca}
\end{figure}
%Four figures displaying hyperparameter search for the second approach. The best estimator is visualized for all hyperparameters as a function of principal components during a grid search with a 5x5 stratified cross validation. The lower plots visualizes the explained variance ratio, both accumulated and stepwise. The dotted lines marks the optimal hyperparameter-combination, while the error bars display the standard deviation.

\subsection{The insightful approach}

Lastly, we turn to the insightful approach, which involves $418$ bad and $172$ good candidates. However, in contrast to the two other datasets, the majority of the entries are labelled as bad candidates.

The grid search for the optimal number of principal components is visualized in figure \ref{fig:03-pca}. Interestingly, we find that all models experience high scores for just a few principal component, where $1$ principal component earns at least $0.875$ in score for all evaluation metrics. This information was also revealed for an earlier 2D-visualization of a scatter plot showing the two most important principal components in figure \ref{fig:2dscatterplotpca}, and consequently can make the models find the optimal decision boundary more easily.

Logistic regression experience improvement of all scores for increasing number of principal components, yet only up $5\%$ in scores compared to the $1D$-representation of $1$ principal components. Thus, one can argue if the increase in performance is worth it considering a one-dimensional representation with just a few percentage loss of performance. However, with multiple principal components, we find the largest increase in precision, which is a sign that the one-dimensional representation tend to wrongly predicts candidates as good when they are in fact bad. The decision tree and the random forest models exhibit best performance for just a few principal components, and experience considerable overfitting for larger values. Gradient boost, in contrast to the two other approaches, also experience best performance for a few principal components.

The optimal hyperparameters are summarized in table \ref{tab:03-pca}, where all models exhibit high evalution metrics. Importantly, we find the difference in number of principal component as most prominent, where logistic regression finds an optimum at $129$ with the f1 score of $0.94$. The decision tree model use only $3$ principal components to achieve a $f1$ score of $0.91$, while random forest needs $11$ principal components to gain a f1 score of $0.94$. Lastly, gradient boost performs optimally at $7$ principal components with a mean f1 score of $0.93$. The relevant hyperparameters was the regularization term for logistic regression, which was set as $10^{-1}$, and the maximum number of iterations as $200$. The decision tree used an maximum depth of $4$, where larger values increased the training accuracy but not any other metric. Random forest was set with maximum depth of $7$, and gradient boost was given $4$.

\clearpage
\begin{figure}[ht!]
  \begin{subfigure}[b]{1.0\textwidth}
    \centering
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-insightful-approach-176-label.tex}
  \end{subfigure}
  \par\bigskip
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-insightful-approach-176-LOG.tex}
    \caption{}
    \label{fig:q3-LOG}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-insightful-approach-176-DT.tex}
    \caption{}
    \label{fig:q3-DT}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-insightful-approach-176-RF.tex}
    \caption{}
    \label{fig:q3-RF}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-insightful-approach-176-GB.tex}
    \caption{}
    \label{fig:q3-GB}
  \end{subfigure}

  \vspace*{-130mm}
  \caption{{Four figures displaying hyperparameter search for the insightful approach. The best estimator is visualized for all hyperparameters as a function of principal components during a grid search with a $5\times5$ stratified cross validation, and the dotted lines marks the optimal hyperparameter-combination. Train stands for normal training accuracy, while test is the balanced accuracy on the test set. Precision, recall and f1 scores are based on the test set. The upper limit of principal components is decided by the explained accumulated variance at $95\%$, while the optimal model is found by using the $f1$ score.}}
  \label{fig:03-pca}
\end{figure}


\begin{table}[!ht]
\centering
\caption{A table of the optimal number of principal components and the respective scores (standard deviation), as visualized in the dash-dotted line in figure \ref{fig:03-pca}.}
\label{tab:03-pca}
\noindent\makebox[\textwidth]{
\begin{tabular}{M{1.0cm} M{1.0cm} M{2.0cm} M{2.0cm}M{2.0cm}M{2.0cm} }
  \hline
  \hline
   Model & PC & Mean test & Mean precision & Mean recall  & mean f1\\
  \hline
  LOG & $129$ & $0.96(0.018)$ & $0.93(0.041)$ & $0.96(0.036)$ & $0.94(0.025)$ \\
  DT & $3$    & $0.94(0.025)$ & $0.91(0.048)$ & $0.92(0.050)$ & $0.91(0.032)$ \\
  RF & $11$   & $0.96(0.019)$ & $0.93(0.039)$ & $0.95(0.040)$ & $0.94(0.024)$ \\
  GB & $7$    & $0.95(0.023)$ & $0.92(0.044)$ & $0.94(0.047)$ & $0.93(0.030)$ \\
  \hline
\end{tabular}
}
\end{table}
\begin{comment}
\begin{figure}[!tbp]
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-brute-approach-5-RF\space.pdf}
    \caption{}
    \label{fig:q3-GB}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-brute-approach-5-GB\space.pdf}
    \caption{}
    \label{fig:q3-RF}
  \end{subfigure}

  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-brute-approach-5-DT\space.pdf}
    \caption{}
    \label{fig:q3-DT}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-brute-approach-5-LOG\space.pdf}
    \caption{}
    \label{fig:q3-LOG}
  \end{subfigure}
  \vspace*{-130mm}
  \caption{{Four figures displaying hyperparameter search for the third approach. The best estimator is visualized for all hyperparameters as a function of principal components during a grid search with a 5x5 stratified cross validation. The lower plots visualizes the explained variance ratio, both accumulated and stepwise. The dotted lines marks the optimal hyperparameter-combination, while the error bars display the standard deviation. }}
\end{figure}
\end{comment}
