\chapter{Validation}

A thorough testing procedure is important to find out if the code is working as intentionally. The procedure might reveal the presence or absence of bugs, and as a project grows, it can give an indication if a new implementation breaks the original project. Therefore, we present a test-case scenario to test if a few machine learning algorithms are able predict the correct label. It is the same algorithms that will be used in the following chapters, and it will provide us the opportunity to understand how the algorithm works and to draw parallells between the separate works. The entire work of the validation process can be found in the Github project \textit{predicting-ABO3-structures} \cite{Ohebbi2021a}.

The validation process is a reproduction of Ref. \cite{Balachandran2018}. To be able to draw any parallell to their work, we use the exact same dataset in the beginning phase. It should be noted that even if the computational aspects of the validation is closely related to Ref. \cite{Balachandran2018}, the work eventually diverges in terms of focus. In their work they include a stability analysis using convex hull analysis in DFT calculations from OQMD, however, we will in this work not decide whether a compound is considered stable or not in an atomic configuration, but rather focus on the predictive aspects of the task. Herein, we will refer to the word "cubics" for perovskites in the cubic structure, "noncubics" for perovskites in a structure other than cubic, and "nonperovskites" for all other cases.

\section{The ABO3 dataset}

The data used in the validation process is offered as supplimentary data from Ref. \cite{Balachandran2018}. They provide the entire training data with both features and labels, but only provide the entries (compounds) of the test data. Therefore, it is neccessary to obtain the features for the test set ourself without knowledge if the resulting test set is identical with Ref. \cite{Balachandran2018}.

The training dataset in question contains $390$ experimentally reported ABO$_3$ compounds. All compounds are charged balanced, and for every compound there is a feature explaining which structure the compound takes, either being a cubic perovskite, perovskite, or not a perovskite at all. Off the $390$ compounds, there are $254$ perovskites and $136$ non-perovskites. Of the $254$ perovskites, $232$ takes a non-cubic perovskite structure while only $22$ takes the cubic perovskite structure. Consequently, this will be visualized by two columns named Perovskite, which represents if a compound is either perovskite (1) or not perovskite (-1), and Cubic, which represents if a compound is cubic perovskite (1), non-cubic perovskite (-1), or not perovskite(0).

The original training dataset consists of $41$ unique $A$ atoms and $55$ unique $B$ atoms. To generate the test set, we implement all different combinations that are eligible with a total of ($VI$) oxidation number for the $A+B$ atoms. The resulting test data contains $625$ entries and is considerable larger than the training data.

\subsection{Features}

There are in total 9 features we can train a model on. Many of the features are based on the Shannon ionic radii \cite{Shannon1976}, which are estimates of an element's ionic hard-sphere radii extracted from experiment. They are dimensionless numbers, and are frequently used in studies involving perovskite structures of materials since they can be a measurement of the ionic misift of the B atom. This can be used to find the deviation of the structure from an ideal cubic geometry. The octahedral factor for an ABO$_3$ solid is known as
\begin{align}
  O = \frac{r_b}{r_O},
\end{align}
where $r_b$ and $r_O$ are the Shannon radii for the B-atom and oxygen ($r_O = 1.4\text{\AA}$), respectively. If the octahedral factor is $O=0.435$, it corresponds to a hard-sphere closed-packed arrangement where $B$ and $O$ ions are touching, while a six-fold coordination appear to require $0.414 < O < 0.732$ according to empirical studies \cite{Zhang2007}. $O$, $r_A$ and $r_b$ are represented as features in our data set. We can also compute the Goldschmidt tolerance factor \cite{Goldschmidt1926}, which is defined as
\begin{align}
  t = \frac{r_A + r_O}{\sqrt{2}(r_A+r_O)}.
\end{align}
\noindent The tolerance factor favors the following structures in the interval:
\begin{itemize}
  \item $t>1$: Hexagonal nonperovskite.
  \item $0.9 < t < 1.0:$ Cubic perovskite.
  \item $0.75 < t < 0.9:$ Orthorombic perovskite.
  \item $t < 0.75:$ Not a perovskite.
\end{itemize}
\noindent If the tolerance factor is exactly $t=1$, the structure is known as perfectly cubic and is free for any structural alterations.

Furthermore, the Shannon radii $r_A$ and $r_B$ can be directly correlated with the structure. Perovskites require $r_A > r_B$, and that A-atoms are in a 12-fold coordinated site if $r_A > 0.9\text{\AA}$. A-atoms also occur in a sixfold coordinated site if $r_A < 0.8\text{\AA}$ and $r_B >0.7\text{\AA}$.

From bond valence theory we can find the valence of an ion to be the sum of valences, that is
\begin{align}
  V_i &= \sum_i \nu_{ij} \\
  &= \sum_i \frac{\exp(d_o - d_{ij})}{b} \label{eq:bondlength},
\end{align}
where $d_{ij}$ is the bond length while $d_0$ and $b$ are parameters from experimental data. The bond length can be found from \ref{eq:bondlength} given the general value $b=1.4\text{\AA}$ and $d_0$, that can be found from Zhang \textit{et al}. database \cite{Zhang2007}. The valence of an ion is associated with its neighboring ions and the chemical bonds, and therefore the band length $d_{AO}$ and $d_{BO}$ are included in the data set.

The two last features originates from the Mendeleev numbers of Villars \textit{et al.} \cite{Villars2004} for the A- and B atom, MA and MB, respectively. The given values positions the elements in structurally similar groups. This means that he groups the elements in the following interval.

\begin{itemize}
  \item s-block $\in \{1,10\}$.
  \item Sc $ = 11$.
  \item Y  $ = 12$.
  \item f-block $\in \{13,42\}$.
  \item d-block $\in \{43,66\}$.
  \item p-block $\in \{67,10\}$.
\end{itemize}

\begin{figure}[ht!]
  \centering
  \includegraphics{../predicting-perovskites/reports/figures/parallel_coordinates/cubicCase.pdf}
  \vspace*{-130mm}
  \caption{A parallel coordinate plot of the perovskite dataset, where the color is given by the cubic label of an entry. A cubic perovskite is labelled as 1, while only a perovskite as -1, or not a perovskite as 0.}
  \label{fig:plcp}
\end{figure}

\noindent The dataset and its features have been visualized in the parallel coordinate \cite{Inselberg1985} figure \ref{fig:plcp}, and reveals several trends already. We can observe that an entry's A atom should preferably have a small Mendeley number (MA) and a large bond length $d_{AO}$. Yet, perhaps the most clear trend is the tolerance factor that should be around $1$. A parallel coordinate plot can easily show trends, but becomes harder to interpret for many features and entries with a growing amount of overlapping. The trend for t values becomes harder to interpret when comparing with the distribution of entries for t-values in figure \ref{fig:hist_perov_t}. From the distribution we learn that there is an overlap of perovskites or not for tolerance factor values in the interval $0.8$ to $1.0$, but the label perovskite is in general preferred. Additionally, we see that the interval ${q1,q3}$ for the label (1) completely overlaps with the corresponding interval for non-perovskites (-1), with very few entries outside of the intervals. This is presumably due to easy labelling for entries that rest outside of the intervals, but the exclusion of entries could potentially alter any model due to not enough entries. %From the data, it is clear that any machine learning algorithm will only predict perovskites inside of the interval {0.8, 1.10}

\begin{figure}[ht!]
  \centering
  \includegraphics{../predicting-perovskites/reports/figures/histogram_training_data_t.pdf}
  \vspace*{-130mm}
  \caption{The $t$-distribution of entries in the dataset for perovskite (1) or not (-1). The upper part for perovskite (1) displays minimum value at $0.80$, q1 at $0.90$, median at $0.93$, q3 at $0.97$ and max at $1.10$. For the non-perovskites (-1), the minimimum is at $0.73$, q1 at $0.87$, median at $0.99$, q3 at $1.12$ and max at $1.47$.}
  \label{fig:hist_perov_t}
\end{figure}

\section{Implementation}

The machine learning classifiers that we will utilize are logistic regression, random forest and gradient boost. The implementation is optimized for adding new algorithms from libraries such as sklearn \cite{Pedregosa2012} or imblearn \cite{Lemaitre2016} with only few lines of code. This is in particular visualized through the implementation of the current algorithms in code listing \ref{lst:insertAlgorithms}, since a special emphasis on reuse and simplicity of code is in focus of this project.

\lstinputlisting[language=Python, caption={}, label={lst:insertAlgorithms}]{results/code-listings/InsertAlgorithms.tex}

The predictions are divided into two parts; perovskite predictions and cubic perovskite predictions. We apply the standard scaler of sklearn \cite{Pedregosa2012} to the training data, followed up by a search of optimal hyperparameters using a $5x5$-stratified cross-validation. This ensures that the percentage of perovskites (cubic perovskites) or not are the same in every subsample in a cross validation as it is in the entire dataset. This is not neccessarily important for the perovskites predictions due to $65/35 \%$ of perovskites or not, but becomes significant for the cubic case where the ratio of cubic perovskites or not are $91/9\%$.

\section{Results and discussion}

Utilising four different classifiers on two different tasks, starting with prediction of perovskite and then prediction of the predicted perovskites into cubic perovskite or only perovskites, yields in total eight different models. We search for optimal parameters for each of the two tasks. Decision tree, random forest and gradient boost share the range of maximum depth starting from $1$ and up to $8$, while we optimise logistic regression for regularization parameters in the range of $10^{-3}$ to $10^5$.

\subsection{Technical details on ML classifiers}

\subsubsection{Perovskite case}
We first consider the ML classification of known ABO$_3$ into perovskite or nonperovskites. A search for optimal hyperparameters using scikit-learn's grid search scheme \cite{Pedregosa2012} reveals the following table with optimal parameters \ref{tab:perovskite-optimal}. We find that all classifiers have for all scores at least $90\%$ accuracy, with gradient boost performing sligthly better than the rest.

\begin{table}[!ht]
\centering
\caption{Table with corresponding best estimators during a grid search scheme for predicting perovskites or not. The test score is here referred to as a balanced accuracy score, and we list all standard deviations in paranthesis.}
\label{tab:perovskite-optimal}
\noindent\makebox[\textwidth]{
\begin{tabular}{M{2.0cm} M{2.5cm} M{2.5cm} M{2.5cm} M{2.5cm} }
  \hline
  \hline
   Model & Mean test & Mean precision & Mean recall & Mean f1  \\
  \hline
  LOG & $0.90(0.041)$ & $0.92(0.034)$ & $0.95(0.023)$ & $0.94(0.024)$ \\
  DT  & $0.90(0.029)$ & $0.93(0.029)$ & $0.95(0.033)$ & $0.94(0.017)$ \\
  RF  & $0.93(0.023)$ & $0.96(0.025)$ & $0.95(0.024)$ & $0.95(0.015)$ \\
  GB  & $0.94(0.025)$ & $0.96(0.025)$ & $0.94(0.036)$ & $0.95(0.019)$ \\
  \hline
\end{tabular}
}
\end{table}

The parameter search is visualized in figure \ref{fig:perovskite-params} for all four models. For logistic regression, we find that by increasing the regulariation the model becomes more general due to a better compromise between precision and recall. For the decision tree model, we find that the optimal maximum number of depth should be $4$. It is clear that the training accuracy increases for larger depth, yet the other test evaluation metrics does not improve, causing the model do be prone for overfitting. Random forest, on the other hand, experience an improvement in scores for all metrics with increasing depth, except for the recall. We find a high recall for all models with an underfitting model due to a imbalanced dataset with a larger amount of perovskites than nonperovskites, and recall is the metric for evaluating if perovskites are correctly predicted. Random forest experience a good compromise between recall and precision with maximum depth at $7$. Lastly, we find the optimal depth of gradient boost as $4$, whereas larger values tend towards overfitting.

\clearpage
\begin{figure}[!tbp]
  \begin{subfigure}[b]{1.0\textwidth}
    \centering
    \input{../predicting-solid-state-qubit-candidates/reports/figures/pca-scores/03-insightful-approach-176-label.tex}
  \end{subfigure}
  \par\bigskip
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-perovskites/reports/figures/grid-scores/LOG-cubic:False.tex}
    \caption{}
    \label{fig:per-LOG}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-perovskites/reports/figures/grid-scores/DT-cubic:False.tex}
    \caption{}
    \label{fig:per-DT}
  \end{subfigure}

  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-perovskites/reports/figures/grid-scores/RF-cubic:False.tex}
    \caption{}
    \label{fig:per-RF}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \input{../predicting-perovskites/reports/figures/grid-scores/GB-cubic:False.tex}
    \caption{}
    \label{fig:per-GB}
  \end{subfigure}
  \vspace*{-130mm}
  \caption{{Four figures displaying hyperparameter search for predicting perovskites or nonperovskites. The best estimator is visualized for all hyperparameters as a function of (a, b and c) max depth or (d) regularization strength during a grid search with a 5x5 stratified cross validation. The dotted lines marks the optimal hyperparameter-combination, while the error bars visualizes the standard deviation. }}
  \label{fig:perovskite-params}
\end{figure}

A total of $25$ classification attempts were done, and we choose the cubic training dataset based on perovskites that the models were able to predict correctly atleast $50\%$ of the time. None of the perovskites were excluded for Random forest and gradient boost due to high correct prediction rate, but $11$ perovskites were wrongly predicted as nonperovskites by the logistic regression, while the number was $4$ for the decision tree model. Importantly, all models were able to predict the cubic perovskites as perovskites, which could potentially alter the further prediction due to a small amount of cubics.

\subsubsection{Cubic perovskite case}


Then, we consider the ML classification of known perovskites into cubic perovskites and noncubic perovskites. Due to a severaly imbalanced dataset with one cubic perovskite for every ten perovskites, we randomly pick perovskites to include in the training set such that the class balance becomes more or less equal to $50:50$ ratio. Thus, we leave out a large part of the data but it is found helpful to reduce the variation of the evaluation metrics. Specifically, this means that the logistic regression is training on a dataset containing $22$ cubics and $20$ noncubics, while the three remaining models train on $22$ cubics and $21$ noncubics.

The optimal combination of hyperparameters during a $5\times 5$ stratified cross validation grid search result in table \ref{tab:cubic-optimal}. We recognize an increase in standard deviation for the metrics as every prediction counts higher due to a small dataset. Interestingly, we find the decision tree model as the best performing model with $0.98$ f1 score, while logistic regression also perform well with $0.96$. Random forest and gradient boost experience a f1 score of $0.93$ and $0.94$, respectively. The relevant hyperparameters found was for logistic regression the regularization term of $0.46$ and number of iterations at $200$. For decision tree, the optimal maximum depth was set as $1$ with increasing deviations for increasing values. Therefore, we believe that the model has potentially set a decision boundary that nearly all entries follow. Random forest and gradient boost performed optimally for the maximum depth of $3$ and $4$, respectively.

We observe that all models experience high scores, but due to the small dataset we need to bear in mind that if we were to add a single datapoint, which would be predicted falsely, could potentially alter the scores up to $5\%$. Therefore, we cannot accurately determine if the models perform but rather find a general trend due to the data present.


\begin{table}[!ht]
\centering
\caption{Table with corresponding best estimators during a grid search scheme for predicting cubic perovskites or only perovskites. The test score is here referred to as a balanced accuracy score, and we list all standard deviations in paranthesis.}
\label{tab:cubic-optimal}
\noindent\makebox[\textwidth]{
\begin{tabular}{M{2.0cm} M{2.5cm} M{2.5cm} M{2.5cm} M{2.5cm} }
  \hline
  \hline
   Model & Mean test & Mean precision & Mean recall & Mean f1  \\
  \hline
  LOG & $0.96(0.073)$ & $0.96(0.080)$ & $0.96(0.123)$ & $0.96(0.085)$ \\
  DT  & $0.97(0.046)$ & $0.96(0.078)$ & $1.00(0.000)$ & $0.98(0.043)$ \\
  RF  & $0.92(0.065)$ & $0.89(0.146)$ & $0.97(0.071)$ & $0.93(0.061)$ \\
  GB  & $0.94(0.083)$ & $0.92(0.104)$ & $0.97(0.071)$ & $0.94(0.074)$ \\
  \hline
\end{tabular}
}
\end{table}


\subsection{Predictions of new compounds}

With the optimal hyperparameters found from the $5\times5$ cross validation, we train the four models on the entire dataset with the labels indicating a perovskite or a nonperovskite. Then we use the each respective cubic training dataset, of varying size due to perovskite misclassifications, to train four new models that will predict if a predicted perovskite will belong to the cubic perovskite class or non at all based on an equal class distributed training set.

\input{results/tikz-plots/validation-tree.tex}

\noindent For the predictions, we input the test set consisting of $625$ unlabelled possible perovskites. The workflow is visualized as a top-down approach in figure \ref{fig:workflow}, where we start with the input of test set. Thus, every model has the same input. For the first branch of each model, we find the green and red circle indicating the number of predicted perovskites or nonperovskite, respectively. Then, we use the predicted perovskites to predict if they belong to the cubic perovskite class (green) or not (red). To simplify the top-down approach, we have added arrows to indicate the direction of predictions.

For the case of prediction perovskites or nonperovskites, we find that logistic regression, decision tree, random forest and gradient boost predict $211$, $243$, $187$ and $206$ as perovskites, respectively. Interestingly, we find decision tree as the one admitting most entries into the perovskite category, but about $60$ of the entries predicted as perovskites are done so with the precision of a coin-flip, that is about $50\%$. The majority of these entries are also based on coin-flips for the other models. We observe that all models agree on classifying $141$ of the initial $625$ entries as perovskites.

We then turn to the prediction of ABO$_3$ compounds into cubic perovskites based on the model's predicted perovskites, which is the second level of branches in figure \ref{fig:workflow}. We find that most of the perovskites are not predicted in the cubic structure, with decision tree and random forest predicting the most cubic perovskites with $31$ and $30$, respectively. Importantly, they agree on $29$ of the predictions, where $17$ of the entries have Pb as A-atom while the rest include K, Rb, Cs, Ba and Sr as A-atom. By including gradient boost in the comparison, we find $7$ out of $8$ cubic perovskites as predicted by gradient boost also in the $29$ cubic perovskites predicted by decision tree and random forest. These predicted cubic perovskites are PbIrO$_3$, PbRuO$_3$, RbBiO$_3$, BaVO$_3$, PbCoO$_3$, PbCrO$_3$ and PbNiO$_3$. Logistic regression, on the other hand, predicts cubic perovskites with the A-atom being one of the alkali metals K, Rb, Cs or the alkaline metal Sr, but disagrees with gradient boost of the choice of B atom since no clear trends have been observed for neither. We believe the randomness of the B-atom originates from when we balanced the training sets, where we removed over $70\%$ of the training set, and consequently removed important distinctions of information.

Thus, we experience similar results as Ref. \cite{Balachandran2018} but with one important distinguishment; none of the models seems to be able to verify their suggestion of any Tl as an eligible A-atom. All models, however, agree on one entry as a cubic perovskite with $1$ in probability, which is RbBiO$_3$.

\section{Conclusion}

In this validation chapter we implemented four algorithms, namely logistic regression, decision tree, random forest and gradient boost to predict if experimental data of ABO$_3$ solids take the perovskite or cubic perovskite, perovskite or nonperovskite structure. Based on this list, we optimalize the models using Scikit-learn's \cite{Pedregosa2012} gridsearch scheme during $5\times 5$ stratified cross validations. We approached the task in two steps; (1) predict perovskites or nonperovskites and consecutively (2) predict cubic perovskites out of the predicted perovskites. For the second step, we balanced the training set of perovskites and cubic perovskites until it was approximately $1:1$ ratio of each by randomly selecting the majority class. Thus, we achieved consistent results but with the loss of data points.

Even if we experienced discrepency in the models and statistical fluctuations of the data, we note that the models were able to independently agree on that $141$ ABO$_3$ compounds were predicted as perovskites. Additionally, all models agreed on one cubic perovskite out of the $141$ perovskites, which was RbBiO$O_3$. However, we found a tendency for all models to prefer the alkali metals K, Rb, Cs or the alkaline metals Ba and Cr for the A-atom in ABO$_3$ compounds that take the cubic perovskite structure.

We note that this work was a reproduction of Ref. \cite{Balachandran2018}, where we initially started with the identical dataset given in the supplementary table. However, we are unable to verify if we are using the same test data due to unavailability of their data and features used for the predictions. We have followed their discussion in regards of generating the test set, and we have made our approach freely distributed under the MIT license on the Github repository \cite{Ohebbi2021a}. We achieve similar results, but with one important distinguishment which is that we do not find the Tl atom as an eligible A-atom for the cubic perovskite structure of ABO$_3$ compounds. On a final note, we believe that the verification of a compound's final structure is up to experimentalists to confirm.
