\chapter{Machine learning}

The enormous amount of data generated in the digital world today is beyond comprehension. In $2019$, more than $500$ hours of video was uploaded to Youtube every second, totaling over $82$ years of content every day\footnote{Source: https://www.youtube.com/intl/no/about/press (Visited 15.02.2021)}. In addition, more than $1.5$ billion web sites exists\footnote{Source: https://www.statista.com/chart/19058/how-many-websites-are-there (Visited 29.03.2021)}.

However, an increasing amount of data comes hand in hand with an increasing demand for knowledge about the data. If we are unable to extract information from the data, the data serves no intention and exists as an excess. Therefore, we need methods to process and automate data analysis, which is what the promises of \textit{machine learning} cover. Machine learning can reveal patterns in data with ease where a human would face difficulties, and use this information to predict or generate new data. Many tools in machine learning are based on probability theory, which can be applied to problems involving uncertainty. Thus, machine learning is also commonly named \textit{statistical learning} \cite{Murphy2012}.

There are mainly two types of machine learning, either \textit{supervised} or \textit{unsupervised} learning. In unsupervised learning we are given inputs $\mathcal{D}=\{\boldsymbol{x}_i\}^N_{i=1}$, where $\boldsymbol{x}_i$ is a training input that has $D$-dimensions that describes each entry, where each dimension is known as a \textit{feature} or a \textit{descriptor}. The features could be exemplified as height or weight, or it could be something complex that has no practical meaning (at least not to humans). Since no features are describing what an entry is, it is up to the tools of machine learning to find patterns in the data and is the essence of unsupervised learning.
In the supervised approach, on the other hand, the model tries to learn a mapping from inputs $\boldsymbol{x}$ to outputs $y$, given a labeled set of pairs $\mathcal{D}=\{(\boldsymbol{x}_i, y_i)\}^N_{i=1}$. The set $\mathcal{D}$ is known as the training set, and $N$ is the number of entries.
The flexibility of the shape of a feature is also shared with the output. It can in principle be anything, but it is mostly assumed that the output is either \textit{categorical} or \textit{nominal} restricted by a finite set $y_i \in \{1,...,\mathcal{C} \}$. The problem is defined as \textit{classification} if the output is categorical, or \textit{regression} if the output is real-valued \cite{Murphy2012}.

%This is, however, outside of the scope of this thesis, since we will solely focus on supervised classification.

\section{Supervised learning}

Supervised learning applied to classification has as goal to learn the target output $y \in \{1,..,\mathcal{C}\}$ from the inputs $\boldsymbol{x}$. The number of classes is $\mathcal{C}$, and depicts if the classification is \textit{binary} ($\mathcal{C}=2$), \textit{multiclass} ($\mathcal{C}>2$), or \textit{multi-label} if the class labels are not mutually exclusive (exemplified with the weather can be both sunny and cold at the same time). Normally, classification is used when the problem is formulated as a multiclass classification, and hereon we will adapt to this formulation as well \cite{Murphy2012}.

In order to be able to learn from data, we will need to formulate a function approximate. Assume $y = f(x) + \epsilon$ for some unknown function $f$ and a random error term $\epsilon$ with mean zero. We can then try to approximate $f$ from a labeled training set, which we can use to make the predictions $\hat{y}=\hat{f}(\boldsymbol{x})$. With the estimated $\hat{f}$ we can make predictions on unlabeled data and achieve a \textit{generalized model}. The estimated function $\hat{f}$ is often considered as a black box, since we are not necessarily interested in the exact shape of the function but rather the predictions.

As simple as the idea behind supervised classification appears, a generalized model remains deeply dependent on the available data. Imagine a training set containing two entries. The first entry is a young and tall person labeled healthy. The other entry is an old and short person labeled sick. The pattern in this simple scenario is abundantly clear, but will face a challenge if it were to predict on a test set containing a person who is young and short. Therefore, it is desirable to compute the probability of an entry belonging to one class. The probability distribution is given by $p(y|\boldsymbol{x}, \mathcal{D})$, where the probability is conditional on the input vector (test set) $x$ and the training set $\mathcal{D}$. If the output is probabilistic, we can compute the estimation to the true label as
\begin{align}
  \hat{y} = \hat{f}(\boldsymbol{x}) = \operatorname*{argmax} f(x) p(y = 1|\boldsymbol{x}, \mathcal{D}),
\end{align}
which represents the most probable class label and is known as the \textit{maximum a posteriori} estimate \cite{Murphy2012}.

\section{Evaluating accuracy of a model}
\label{evaluating accuracy}
%The idea of finding one algorithm that is far more superior than any other algorithm, for all types and sizes of datasets, is of an imaginary sort.
It would be desirable to find one superior model that we could utilize on all types and sizes of datasets. Unfortunately, there is no algorithm that has this property, since one model might be recognized as best on one particular dataset, while others are far better on other datasets. This is known as the \textit{no free lunch theorem} (Wolpert 1996 \cite{Wolpert1996}). The same goes with evaluating the model - there is no metrics that stand alone as the best metric to evaluate a model. Choosing how to actually evaluate a model can be the most challenging part of a statistical learning procedure.

\subsection{Bias-variance tradeoff}

To illustrate a challenge in choosing the correct parameters, we give an example using the mean squared error (MSE) as a \textit{cost function}, which we want to minimize in order to improve the accuracy of the model \cite{Murphy2012}. Assume that our data can be represented by
\begin{align*}
\boldsymbol{y}=f(\boldsymbol{x}) + \boldsymbol{\epsilon},
\end{align*}
where $f(\boldsymbol{x})$ is an unknown function and $\boldsymbol{\epsilon}$ is normally distributed with a mean equal to zero and variance equal to $\sigma^2$. Furthermore, we also assume that the function $f(\boldsymbol{x})$ can be approximated to a model $\boldsymbol{\hat{y}}$, where the model is defined by a design matrix $\boldsymbol{X}$ and parameters $\boldsymbol{\beta}$,

\begin{align*}
\boldsymbol{\hat{y}} = \boldsymbol{X}\boldsymbol{\beta}.
\end{align*}
The parameters $\boldsymbol{\beta}$ are in turn found by optimizing the mean squared error (MSE) via the cost function

\begin{align*}
C(\boldsymbol{X},\boldsymbol{\beta}) =\frac{1}{n}\sum_{i=0}^{n-1}(y_i-\hat{y}_i)^2= E\left[(\boldsymbol{y}-\boldsymbol{\hat{y}})^2\right].
\end{align*}
The cost function can be rewritten as
%\begin{align*}
%C(\boldsymbol{X},\boldsymbol{\beta}) = E\left[(\boldsymbol{y}-\boldsymbol{\hat{y}})^2\right].
%\end{align*}

\begin{align*}
E\left[\left(\boldsymbol{y}-\boldsymbol{\hat{y}}\right)^2\right] &= \frac{1}{n}\sum_i\left(f_i- E\left[\boldsymbol{\hat{y}}\right]\right)^2+\frac{1}{n}\sum_i\left(\hat{y}_i- E\left[\boldsymbol{\hat{y}}\right]\right)^2+\sigma^2 \\
&= E\left[\left(\boldsymbol{f}-E\left[\boldsymbol{\hat{y}}\right]\right)^2\right] + Var\left(\boldsymbol{\hat{y}}\right) + \sigma_{\epsilon}^2
\end{align*}
where $E[\boldsymbol{y}] = \boldsymbol{f}$, $E\left[\boldsymbol{\epsilon}\right] = \boldsymbol{0}$ and $Var\left(\boldsymbol{y}\right) = Var \left(\boldsymbol{\epsilon}\right) = \sigma_{\epsilon}^2$ \cite{Murphy2012}.
\begin{comment}
and since the variance of $\boldsymbol{y}$ and $\boldsymbol{\epsilon}$ are both equal to $\sigma^2$, we can use the relations $E[\boldsymbol{y}] = \boldsymbol{f}$, $ E[\boldsymbol{\epsilon}] = 0 $ and $Var(\boldsymbol{y}) = Var(\boldsymbol{\epsilon}) = \sigma_{\epsilon}^2$. The mean value of $\boldsymbol{\epsilon}$ is by definition equal to zero. In addition, the function $\boldsymbol{f}$ is not a stochastic variable, and the same argument can also be used for $\boldsymbol{\hat{y}}$. This allows us to insert the expression $\boldsymbol{y}$ into the cost function
\begin{align*}
E[(\boldsymbol{y-\hat{y}})^2] &= E[(\boldsymbol{f + \epsilon - \hat{y}})^2].
\end{align*}
By using the infamous trick of both adding and subtracting simultaneously, we arrive at
\begin{align*}
E\left[(\boldsymbol{y}-\boldsymbol{\hat{y}})^2\right] &= E[(\boldsymbol{f + \epsilon - \hat{y}} + E[\boldsymbol{y}] - E[\boldsymbol{y}])^2 ].
\end{align*}

And simply by using the relations mentioned above concerning the expectation value for $\boldsymbol{y}$ and the variances for both $\boldsymbol{y}$ and $\epsilon$, the cost function can be rewritten to:

\begin{align*}
E\left[(\boldsymbol{y}-\boldsymbol{\hat{y}})^2\right] &=  E[(\boldsymbol{f}-E[\boldsymbol{\hat{y}}])^2] + Var(\boldsymbol{\hat{y}}) + \sigma_{\epsilon}^2 \\
 &= \frac{1}{n}\sum_i(f_i- E\left[\boldsymbol{\hat{y}}\right])^2+\frac{1}{n}\sum_i(\hat{y}_i- E\left[\boldsymbol{\hat{y}}\right])^2+\sigma^2_{\epsilon}.
\end{align*}

\end{comment}
The first term on the right-hand side is the squared bias, the amount by which the average of our estimate differs from the true mean, while the second term represents the variance of the chosen model. The last term is the variance of the error $\boldsymbol{\epsilon}$, also known as the irreducible error. In general, an estimated function $\hat{f}$ will never be a perfect estimate for $f$ since we can not reduce the error introduced by $\boldsymbol{\epsilon}$. Therefore, any model will always be restricted to an upper bound of accuracy due to the irreducible error.

\input{theory/tikz-plots/bias-variance.tex}

%The more complex model one has, the lower the bias becomes, and the higher the variance becomes, as seen in \autoref{fig:biasVarianceTradeOff}.
A model with high variance will typically experience larger fluctuations around the true value, while a model with high bias corresponds to a larger error in the average of estimates. This is schematically visualized as a function of model complexity in  \autoref{fig:biasVarianceTradeOff}. If the model is not complex enough due to high bias and low variance, the algorithm can end up not learning the relevant relations between features and output. This is known as \textit{underfitting} \cite{Murphy2012}. On the other hand, a complex model with low bias and high variance might find trends in random noise from the training data instead of the relevant features, resulting in \textit{overfitting} \cite{Murphy2012}. An ideal model would be one that simultaneously achieves low variance and low bias.  Therefore, we have to do a trade-off between how much bias and variance we would like in the model.

\subsection{Accuracy, precision and recall}

Given a model that has dealt with the intricacy of increasing complexity, we would like to evaluate the model's output quality. For a binary supervised classification problem, we can measure the accuracy by finding how many correct predictions have been made. Prediction accuracy can provide a fine initial analysis, but it has some significant drawbacks seen in unbalanced datasets. This can be easily explained with a dataset consisting of a $99:1$ ratio of class, since just guessing the majority class will result in a very high $99\%$ accuracy. Perhaps it is the $1\%$ that is the most important class, thus the accuracy score severely lacks information for the model.

Therefore, we turn to other evaluation metrics such as a \textit{a confusion matrix}. A confusion matrix is a method for measuring the performance of classifiers \cite{Murphy2012}. It is set up as a table with 4 different categories, where two of the categories are the predicted outcomes of the classifier and the two final categories are the true outcomes. An example of a confusion matrix for a binary classifier is shown in \autoref{tab:confusion_matrix}.

\begin{table}[!ht]
  \centering
  \renewcommand\arraystretch{1.5}
  \setlength\tabcolsep{0pt}
  \caption{A confusion matrix for a binary classifier. The entries true positive and true negative on the diagonal of the matrix are correct predictions, while false-positive and false-negative are wrongly made predictions. P and N are the total number of positive and negative predictions, respectively. Similarly, P$'$ and N$'$ are the number of true positive or negative labels, respectively.}
  \label{tab:confusion_matrix}
  \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
    \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft Actual\\ label}} &
      & \multicolumn{2}{c}{\bfseries Predicted label} & \\
    & & \bfseries $1$ & \bfseries $0$ & \bfseries total \\
    & $1$ & \MyBox{True}{Positive} & \MyBox{False}{Negative} & P$'$ \\[2.4em]
    & $0$ & \MyBox{False}{Positive} & \MyBox{True}{Negative} & N$'$ \\
    & total & P & N &
  \end{tabular}
  \end{table}
For the binary confusion matrix, there are two possible predicted outcomes, either positive or negative. This gives rise to some terminology.

\begin{itemize}
\item True Positive (TP): The classifier correctly predicts a positive event.
\item True Negative (TN): The classifier correctly predicts a negative event.
\item False Positive (FP): The classifier incorrectly predicts a positive event when the true event was negative.
\item False Negative (FN):  The classifier incorrectly predicts a negative event when the true event was positive.
\end{itemize} From the confusion matrix one can then start estimating the performance of the model, by calculating different factors, such as \cite{Murphy2012}

\begin{itemize}
\item \textbf{Sensitivity}, also known as the true negative rate, is the ratio of the number of correct negative examples to the number classified as negative. It is defined as
\begin{align}
\text{Sensitivity} = \frac{TN}{TN + FP}.
\end{align}

\item \textbf{Recall}, also known as the true positive rate, is the ratio of the number of correct positive examples to the number classified as positive. A high recall relates to a low false-negative rate and is defined as
\begin{align}
\text{Recall} = \frac{TP}{TP + FN}.
\end{align}

\item \textbf{Precision} is the ratio of correct positive examples to the number of actual positive examples. A high precision relates to a low false-positive rate, and is defined as  \\
\begin{align}
\text{Precision} = \frac{TP}{TP + FP}.
\end{align}
\end{itemize} Similar to the bias-variance tradeoff, it is common to compare the recall with the precision to identify the tradeoff for different thresholds. High scores for both reveal that a classifier returns accurate results combined with returning a majority of all positive results.


Sometimes a classifier can have drastically different values for precision and recall. This leads to another estimator for the performance of a classifier, which is known as the F1-score. The F1-score is defined as the harmonic mean of precision and recall,
\begin{align}
\text{F1-score} = \frac{2\cdot \text{Recall} \cdot \text{Precision}}{\text{Recall} + \text{Precision}},
\end{align}
and can be used to find a good tradeoff between recall and precision. The highest value of the F1-score is $1$ and is considered an ideal classifier, while the lowest is $0$.

However, the F1-score is insensitive to the number of negative predictions. Therefore, an adjustment of the normal accuracy is in place. The name of this metric is called the balanced accuracy, which equally weights how many true positive and true negative,
\begin{align}
  \text{Balanced accuracy} = \frac{1}{2} \left( \frac{\text{TP}}{\text{TP} + \text{FN}} + \frac{\text{TN}}{\text{TN} + \text{FP}} \right),
\end{align}
which makes it particular handy for imbalanced datasets.

We have now only scratched the surface of potential evaluation metrics, and as a final note, we would like to emphasize that it is up to the implementer which evaluation metric one should use. %Therefore, one can consider the choice of metric as a subjective choice.

%Therefore, the amount of evaluation metrics and methods one can implement are vast, and it should be noted that the final note
%On an end note, we repeat that the final evaluation of which metric to choose is up to

\subsection{Cross-validation}
\label{cross-validation}
When evaluating different parameters for models, commonly done in a grid-search scheme, there is an abundant risk of performing an overfit to the test set since we can tweak the parameters of a model so it can perform optimally. To solve this problem, we can exclude a part of the dataset as a validation set (in addition to a test set). Therefore, we can train a model on the training set, and evaluate the parameters on the validation set. After a lot of trial and error and the experiment seems successful, we can do one final evaluation on the test set.

Unfortunately, this reduces the number of samples that can be used for training drastically. A fix for this is to apply \textit{cross-validation} (CV) \cite{Murphy2012}. Cross-validation is a technique used to evaluate predictive models by partitioning the original sample into a training set to train the model, and a test set to evaluate it.

\input{theory/tikz-plots/cv.tex}
\noindent It is common to apply cross-validation into folds, yielding the name of k-fold cross-validation. In k-fold cross-validation, the training set is partitioned into k equal-sized subsamples, as visualized in \autoref{fig:cv}. Of the k samples, a single sample is used as a validation set while the remaining k-1 samples are used as training data. The process is then repeated k-times, such that each of the k subsamples is used as a validation set exactly once. Therefore, all observations are used for both training and validation, and each observation is used for validation exactly once. The k results from the folds can then be averaged to produce an estimate. The subsamples are allowed to have an imbalanced dataset, so that each class is not necessarily represented equally in each fold. Since supervised algorithms tend to weigh each instance equally, this may result in overrepresented classes being favored during the training of the model. Even worse could be the result of a fold where one class is not represented at all, resulting in a model that does not learn how to predict a class at all.

To deal with the vulnerability of imbalanced datasets in CV, one can employ a stratified k-fold cross-validation technique. Stratification is a process that seeks to ensure that each fold is representative of all classes (also named \textit{strata} in this context) in the data, making each fold having approximately equal class representation.

\section{Logistic regression}

Logistic regression, or \textit{logit}, is considered a \textit{soft} classification algorithm, which means that an output of the algorithm is considered to be categorical instead of numerical. Assume we have a dataset with $\boldsymbol{x}_i = {x_{i1}, x_{i2}, .., x_{ip}}$ input data, where we have $p$ predictors for each corresponding output data $y_i$. The outcomes $y_i$ are discrete and can only take certain values or classes. In our case we have two classes with $y_i$ either being equal to $0$ or $1$. Therefore, the probability that a datapoint belongs to either class can be given by the Sigmoid function, %which is meant to represent the likelihood for a given event,

\begin{align*}
p(x) = \frac{1}{1+ e^{-x}} = \frac{e^{x}}{1+ e^{x}}.
\end{align*}

\noindent Furthermore, we have the parameters $\boldsymbol{\beta} = {\beta_1, \beta_2, ..., \beta_p}$ of our fitting of the Sigmoid function, where the probabilities are defined as
\begin{align*}
p(y_i|\boldsymbol{x}_i \boldsymbol{\beta}) = \frac{e^{(\boldsymbol{x}_i \boldsymbol{\beta})}}{1 + e^{(\boldsymbol{x}_i \boldsymbol{\beta})}}.
\end{align*}

\noindent The goal of logistic regression is then to correctly predict the category of a given dataset, which has different outcomes, by using an optimal parameter $\boldsymbol{\beta}$ that maximizes the probability of seeing the observed data. How we find the parameters $\boldsymbol{\beta} = {\beta_1, \beta_2, ..., \beta_p}$ of the model, is to use the principle of \textit{maximum likelihood estimation} (MLE),

% We aim thus at maximizing the probability of seeing the observed data. We can then approximate the likelihood in terms of the product of the individual probabilities of a specific outcome yi, that is

% Under the assumption that every sample x is independent, the likelihood is given by

\begin{align*}
P(\boldsymbol{\beta}) = \prod_{i=1}^{n} \left[ p(y_i=1|\boldsymbol{x}_i \boldsymbol{\beta}) \right]^{y_i} \left[1- p(y_i=1|\boldsymbol{x}_i \boldsymbol{\beta}) \right]^{1-y_i},
\end{align*}
\noindent where we obtain the log-likelihood function, which is easier to work with, since the log-likelihood turns the exponentials into summations,% \cite{morten1}. \\

\begin{align*}
C(\boldsymbol{\beta}) = \sum_{i=1}^n \left(y_i\left(\boldsymbol{x}_i \boldsymbol{\beta}\right) - \log{ \left(1 + \exp \left(\boldsymbol{x}_i \boldsymbol{\beta}\right)\right)}  \right).
\end{align*}

\noindent Finally, we choose our cost function as the \textit{cross-entropy}, which is defined as the negative log-likelihood,

% Note that maximising the logarithm of a function is equivalent to maximising the function itself.
% Thus taking \beta to maximise the log-likelihood is equivalent to maximising the likelihood itself.
% Finally, we take our cost function to be the so called cross-entropy which is defined as the negative log-likelihood.

%
\begin{align*}
C(\boldsymbol{\beta}) = - \sum_{i=1}^n \left(y_i\left(\boldsymbol{x}_i \boldsymbol{\beta}\right) - \log{ \left(1 + \exp \left(\boldsymbol{x}_i \boldsymbol{\beta}\right)\right)}  \right).
\end{align*}

\noindent To maximize the accuracy and precision of the logistic regression model, we need to find the optimal parameters $\boldsymbol{\beta}$ by minimizing the cross-entropy.

\subsection{Stochastic gradient descent}

One common numerical method for finding the minimum of a function is \textit{stochastic gradient descent} (SGD). The fundamental idea of SGD comes from the observation that the cost function can be written as a sum over $n$ data points $\{\boldsymbol{x}_i\}_{i=1}^{n}$,
\begin{align*}
C(\boldsymbol{\beta}) = \sum_{i=1}^{n} c_i(\boldsymbol{x}_i,\beta).
\end{align*}

\noindent We can compute the gradient as
\begin{align*}
\nabla_{\beta} C(\boldsymbol{\beta}) =  \sum_{i=1}^{n} \nabla_{\beta} c_i(\boldsymbol{x}_i,\beta)
\end{align*}

\noindent Then, it is possible to introduce randomness by only taking the gradient on a small interval of the data, called a minibatch. With $n$ total data points, and $M$ datapoints per minibatch, the number of mini-batches is then $\frac{n}{M}$.

The idea is now to approximate the gradient by replacing the sum over all data points with a sum over the data points in one of the mini-batches picked at random in each gradient descent step,

\begin{align*}
\nabla_{\beta} C(\boldsymbol{\beta}) =  \sum_{i=1}^{n} \nabla_{\beta} c_i(\boldsymbol{x}_i,\beta) \to \sum_{i \in B_k}^{n} \nabla_{\beta} c_i(\boldsymbol{x}_i,\beta),
\end{align*}

 \noindent where $B_k$ is the set of all mini-batches, with $k=1, ..,\frac{n}{M}$. One step of gradient descent is then defined by

\begin{align*}
\beta_{j+1} = \beta_j - \gamma_j \sum_{i \in B_k}^{n} \nabla_{\beta} c_i(\boldsymbol{x}_i,\beta)
\end{align*}

\noindent where $k$ is picked at random with equal probability from $[1, \frac{n}{M}]$ and $\gamma_j$ is the step length. An iteration over the number of mini-batches $(\frac{n}{M})$ is commonly referred to as an epoch. Thus, it is typical to choose a number of epochs and for each epoch iterate over the number of mini-batches.

\section{Decision trees}
Classification and regression trees (CART), also called decision trees, are one of the more basic supervised algorithms. They can be used for both regression and classification tasks, but we will for the relevancy of this work provide a special emphasis on classification trees.  %The strength of decision trees lays within the simplicity that allows to build more complex networks.
%We will in this section provide special emphasis to classification trees, but with some remarks to the regression trees to provide a brief perspective of distinctions.

The idea behind decision trees is to find the features that contain the most information regarding the target and then split up the dataset along the values of these features. This feature selection enables the target values for the resulting underlying dataset to be as \textit{pure} as possible, which means the dataset only contains one class \cite{Murphy2012}. The features that can reproduce the best target features are normally said to be the most informative features.

A decision tree can be divided into a \textit{root node}, \textit{interior nodes}, and the final \textit{leaf nodes}, commonly known as \textit{terminal nodes}. The nodes are connected by \textit{branches}. The decision tree is able to learn an underlying structure of the training data and can, given some assumptions, make predictions on unseen observations. These predictions are based on the information stored in the leaf nodes in the tree.

\input{theory/tikz-plots/decision-tree.tex}
\noindent The process behind a decision tree can be seen as a top-down approach. First, we make a leaf provide the classification of a given instance. Then, a node specifies a test of some attribute of the instance, while a branch corresponds to a possible value of an attribute. Subsequently, the instance moves down the tree branch corresponding to the value of the attribute. Then the steps can be repeated for a new subtree rooted at the new node.

A classification tree differs from a regression tree by the response of the prediction, since it produces a qualitative response rather than a quantitative one. The response is given by the most commonly occurring class of training observations specified by the attribute of the node. A schematic representation of a classification tree is visualized in \autoref{fig:decision-tree}. %Thus, the interpretation process includes both the class prediction corresponding to a particular terminal node region, but also in the class proportions among the training observations that fall into that region.

\subsection{Growing a classification tree}

In growing a classification tree, a process called recursive binary splitting is applied. This involves two steps:

\begin{enumerate}
  \item Split the set of possible values $(x_1, x_2,...,x_p)$ into $J$ distinct non-overlapping regions $R_1, R_2, ..., R_{J}$.
  \item If an observation falls within the region $R_J$, we make the prediction given by the most commonly occurring class of training observations in $R_{J}$.
\end{enumerate}
The computational aspect of recursively doing this for every possible combination of features does not defend its use, and therefore the common strategy is to use a top-down approach. Binary splitting begins at the top of the tree and consecutively splits the \textit{predictor space}, which is a space that describes all possible combinations of the features in the dataset. This is indicated by two new branches further down the tree. It should be noted that the top-down approach is a greedy approach since the best split is made at each step of the tree-growing process, instead of trying to pick a split that will lead to a better tree in a future step.

We can define a \textit{probability density function} (PDF) $p_{mk}$ that represents the number of observations $k$ in a region $R_m$ with $N_m$ observations. This likelihood function can be represented in terms of observations of a class in region $R_m$ as
\begin{align}
  p_{mk} = \frac{1}{N_m} \sum_{x_i \in R_m} I(y_i = k),
\end{align}
where the \textit{indicator} $I$ function equals zero if we misclassify and one if we classify correctly. Therefore, we can define the splitting of the nodes by the misclassification error
\begin{align}
  m_{e} = \frac{1}{N_m} \sum_{x_i \in R_m} I(y_i \neq k) = 1 - p_{mk}.
\end{align}
However, other methods exists such as the Gini index
\begin{align}
  g = \sum_{k=1}^{K} p_{mk} (1-p_{mk})
\end{align}
and the information entropy
\begin{align}
  s = - \sum_{k=1}^{K} p_{mk} \log p_{mk}.
\end{align}
The two latter approaches are more sensitive to node purity than the misclassification error, i.e. only containing one class, and are generally preferred \cite{Murphy2012} for the splitting of the nodes in a decision tree.

\subsection{Classification algorithm}
The CART algorithm splits the data set in two subsets using a single feature $k$ and a threshold $t_k$. The pair of quantities $(k,t_k)$ that constitute the purest subset using the Gini factor $G$ results in the cost function
\begin{align}
  C(k, t_k) = \frac{m_{\text{left}}}{m}G_{\text{left}} + \frac{m_{\text{right}}}{m}G_{\text{right}},
\end{align}
where $G_{\text{left}}$ ($G_{\text{right}}$) measures the impurity of the left (right) subset and $m_{\text{left}}$  ($m_{\text{right}}$) is the number of instances on the left (right) subset. The algorithm tries to minimize the cost function to find the pair $(k,t_k)$ by splitting the training set in two, and then following the same logic for the next subsets. It will continue to do this recursively until it reaches the maximum depth hyperparameter, or if the next split does not reduce impurity.

\subsection{Pruning a tree}

A decision tree has the ability to turn into a very complex model, making it prone to overfitting. Pre-pruning is a method that stops the growth of a tree if the decrease in error is not sufficient to justify an increasingly complex model by adding an extra subtree. However, this method should not be implemented for models with a large number of features, since features with small predictive powers might be extensively removed which might result in a tree without any splits at all \cite{Murphy2012}. Post-pruning, or just pruning, is the standard method that involves growing the tree to full size, and then prune the tree by cutting branches. To determine how much to prune it, we can use a cross-validated scheme to evaluate the number of terminal nodes that have the lowest error.


%However, this implementation has the liability for features that have small predictive power as this might cause a model without any splits at all.
\subsection{Pros and cons of decision trees}

Decision trees have several clear advantages compared to other algorithms. They are easy to understand and can be visualized effortlessly for small trees. The algorithm is completely invariant to the scaling of the data since each feature is processed separately. Additionally, decision trees can handle both continuous and categorical data and can model interactions between different descriptive features.

As auspicious as the advantages of decision trees seems, they are inevitably prone to overfitting and hence do not generalize well. Even with pre-pruning, post-pruning and setting a maximum depth of terminal nodes, the algorithm is still prone to overfit \cite{Guido2016}. Another important issue concerns training on unbalanced datasets where one class occurs more frequently than other classes, since this will lead to biased trees because the algorithm will favor the more occurring class. Furthermore, small changes in the data may lead to a completely different tree. Many of these issues can be addressed by using ensemble methods such as either bagging, random forest, or boosting, and can result in a solid improvement of the predictive performance of trees.

\section{Ensemble methods}

By using a single decision tree, we often end up with an overfitted model that possesses a high variance. Luckily, we can apply methods that aggregate different machine learning algorithms to reduce variance. If each of the algorithms gets slightly different results, as they learn different parts of the data, we can combine the results into something that is better than any algorithm alone. These approaches fall under the category of ensemble methods and will be elaborated upon in this section. % to construct complex forests, or perhaps more appropiately named \textit{jungles}.

\subsection{Bagging}

\textit{Bootstrap aggregation}, or just \textit{bagging}, is an ensemble method that involves averaging many estimates \cite{Murphy2012}. If we have $M$ trained trees on different subsamples of the data, chosen randomly, we can compute the ensemble
\begin{align}
  f(\boldsymbol{x}) = \frac{1}{B} \sum_{b=1}^B f_b(\boldsymbol{x}),
\end{align}
where $f_b$ is the $b$'th tree. Simply re-running the same algorithm on different subsamples can result in a small variance reduction compared to a single tree due to highly correlated predictors, which showcase the need for better approaches.

\textit{Random forests} provide an improvement of normal bagged trees by choosing a random sample of $m$ predictors as split candidates from the full set of $p$ predictors. The split is restricted in choosing only one of the $m$ predictors, which are normally chosen as either $m \approx \sqrt{p} $ or $m \approx \log{p}$. This means that at each split in a tree, the algorithm is restricted to a very small portion of the available predictors. %The specific about the algorithm can be found in Algorithm~\autoref{alg:randomforest}.

\begin{algorithm}[H]
\SetAlgoLined
 \For{For $b = 1$ : $B$}{
  Draw a bootstrap sample from the training data\;
  Select a tree $T_b $ to grow based on the bootstrap data\;
  \While{node size smaller than maximum node size}
  {
   Select $m \leq p$ variables at random from $p$ predictors\;
   Pick the best split point among the $m$ features using CART algorithm and create a new node\;
   Split the node into daughter nodes\;
   }
 }
 Output the ensemble of trees $\{T_b\}_{b=1}^B$ and make predictions
 \caption{Random forest algorithm.}
 \label{alg:randomforest}
\end{algorithm}

\noindent By introducing randomness into the model, we arrive at a surprisingly capable model that has a high predictive accuracy \cite{Caruana2006}. This can be exemplified by supposing that there is one strong predictor in a dataset, together with several other fairly strong predictors. Most of the trees will use this strong predictor at the top split, which means that the bagged trees will look quite similar to each other and will have highly correlated predictions.

However, even with higher prediction accuracy, it comes as a compromise since we lose the easy ability of model interpretation. A single tree can be easy to understand, but the interpretation of a huge jungle of trees does not necessarily seem appealing for even an experienced data scientist. Furthermore, a random forest does not substantially reduce the variance as averaging many uncorrelated trees would do, as we will soon find out.

\subsection{Boosting}

Boosting is an ensemble method that fits an additive expansion in a set of elementary basis functions \cite{Murphy2012}. The basic idea is to combine several weak classifiers, that are only just better than a random guess, in order to create a good classifier. This can be done in an iterative approach where we apply a weak classifier to modify the data. For each iteration, we make sure to weigh the observations that are misclassified with a factor. The method is known as adaptive boosting since the algorithm is able to adapt during the learning process.

In \textit{forward stagewise additive modeling} we want to find an adaptive model
\begin{align}
  f_M (\boldsymbol{x}) = \sum_{m=1}^M \beta_m G_m(\boldsymbol{x}; \gamma_m),
\end{align}
where $\beta_m$ are expansion parameters that will be determined in a minimization process, and $G_m(\boldsymbol{x};\gamma_m)$ are functions of the multivariable parameter $\boldsymbol{x}$ that are described by the parameters $\gamma_m$. We will in this example consider a binary classification problem with the outcomes $\gamma_i \in \{-1,1\}$ where $i=0,1,2,...,n-1$ are the set of observables. The predictions are produced by the classification function $G(\boldsymbol{x})$. The error rate of the training sample is given as
\begin{align}
  \overline{\text{err}} = \frac{1}{n} \sum_{i=0}^{n-1} I(\hat{y}_i \neq G(\boldsymbol{x}_i)).
\end{align}

\noindent After defining a weak classifier, we can apply it iteratively to repeatedly modified versions of the data producing a sequence of different weak classifiers $G_m(\boldsymbol{x})$. The iterative procedure can be defined as
\begin{align}
  f_m(\boldsymbol{x}) = f_{m-1}(\boldsymbol{x}) + \beta_mG_m(\boldsymbol{x}),
  \label{eq:fsam}
\end{align}
where the function $f_M(\boldsymbol{x})$ will be expressed in terms of
\begin{align}
  G(\boldsymbol{x})=\text{sign}\sum_{i=1}^M \alpha_mG_m(\boldsymbol{x}),
\end{align}
where $\alpha_m$ is the weight that descibes the contribution from the weak classifier $G_m(\boldsymbol{x})$.
The main idea is that we do not go back and adjust earlier parameters, which is why this is called \textit{forward} stagewise additive modeling.

We can demonstrate a binary classification example using the exponential cost function that leads to the \textit{discrete AdaBoost} algorithm \cite{Friedman2000} at step $m$,
\begin{align}
  %C(\boldsymbol{y},\boldsymbol{f}) = \sum_{i=0}^{n-1} \exp (-\hat{y}_i(f_{m-1}(\boldsymbol{x}_i) + \beta G(\boldsymbol{x}_i))).
  C(\boldsymbol{y},\boldsymbol{f}) = \sum_{i=0}^{n-1} w_i^m \exp(-\hat{y}_i\beta G(\boldsymbol{x}_i)),
\end{align}
where $w_i^m = \exp(-\hat{y}_if_{m-1}(\boldsymbol{x}_i))$ is the weight of the corresponding observable $i$.
%\begin{align}
%  C(\boldsymbol{y},\boldsymbol{f}) = \sum_{i=0}^{n-1} w_i^m \exp(-\hat{y}_i\beta G(\boldsymbol{x}_i)).
%\end{align}
We can optimize $G$ for any $\beta>0$ with
\begin{align}
  G_m(\boldsymbol{x}) = \text{sign} \sum_{i=0}^{n-1} w_i^m I(\hat{y}_i \neq G(\boldsymbol{x}_i)).
\end{align}
This is the classifier that minimize the weighted error rate in predicting $y$. Furthermore, we can rewrite the cost function to
\begin{align}
    %C &= \exp(-\beta) \sum_{\hat{y}_i=G(x_i)} w_i^m + \exp (\beta) \sum_{\hat{y}_i \neq G(\boldsymbol{x}_i)} w_i^m \\
    C &= (\exp(\beta)-\exp(-\beta)\sum_{i=0}^{n-1} w_i^m I(\hat{y}_i \neq G(x_i)) + \exp(-\beta)\sum_{i=0}^{n-1}w_i^m.
    \label{eq:exponentialcost}
\end{align}
Substituting $G_m$ into $C$ and solving for $\beta$, we obtain
\begin{align}
  \beta_m = \frac{1}{2} \log \frac{1 - \overline{\text{err}}}{\overline{\text{err}}},
\end{align}
with the error redefined as
\begin{align}
  \overline{\text{err}} = \frac{1}{n} \frac{ \sum_{i=0}^{n-1} w_i^m I(\hat{y}_i \neq G_m(\boldsymbol{x}_i)) }{\sum_{i=0}^{n-1} w_i^m}.
\end{align}
Finally, this leads to an update of $f_m(\boldsymbol{x})$ as defined in \autoref{eq:fsam} and the weights at the next iteration becomes
\begin{align}
  w_i^{m+1} = w_i^m \exp (-\hat{y}_i \beta_m G_m(\boldsymbol{x}_i)).
\end{align}
With the above definitions, we can define the discrete Adaboost algorithm in Algorithm \ref{alg:discreteAdaboost}.

\begin{algorithm}[H]
\SetAlgoLined
  Initialize weights $w_i = 1/n, \quad i=0,...,n-1$, such that $\sum_{i=0}^{n-1}w_i = 1$\;
 \For{$m = 1$ : $M$}{
  Fit the classifier $f_m (\boldsymbol{x}) \in \{-1,1\}$ using weights $w_i$ on the training data\;
  Compute the error $\overline{\text{err}} = \frac{1}{n} \frac{ \sum_{i=0}^{n-1} w_i^m I(\hat{y}_i \neq G_m(\boldsymbol{x}_i)) }{\sum_{i=0}^{n-1} w_i^m}$ \;
  Define a quantity $\alpha_m = \log \big[(1-\overline{\text{err}_m})/\overline{\text{err}}_m$ \big] \;
  Set new weights to $w_i \leftarrow w_i \exp(\alpha_m I(y_i \neq G(\boldsymbol{x}_i)))$\;
 }
 Compute the new classifier $G(\boldsymbol{x}) = \sum_{i=0}^{n-1} \alpha_m I(y_i \neq G(\boldsymbol{x}_i))$;
 \caption{Discrete Adaboost algorithm.}
 \label{alg:discreteAdaboost}
\end{algorithm}

\noindent It is possible to apply different cost functions resulting in a variety of boosting algorithms. AdaBoost is an example with the cost function in \autoref{eq:exponentialcost}. But instead of deriving new versions of boosting based on different cost functions, we can find one generic method. This approach is known as \textit{gradient boosting} \cite{friedman2001}. Initially, we want to minimize
\begin{align}
  \hat{\boldsymbol{f}} = \argmin_f L(\boldsymbol{f}),
\end{align}
where $\boldsymbol{f} = \big(f(\boldsymbol{x}_1), ..., f(\boldsymbol{x}_N)) \big)$ are the parameters of the models, and $L$ is a chosen loss function.

This can be solved stagewise using gradient descent. At step $m$, let $\boldsymbol{g}_m$ be the gradient evaluated at $f(\boldsymbol{x}_i) = f_{m-1}(\boldsymbol{x}_i)$:
\begin{align}
  \boldsymbol{g}_m(\boldsymbol{x}_i) = \Bigg[ \frac{\partial L(y_i, f(\boldsymbol{x}_i))}{\partial f(\boldsymbol{x}_i)} \Bigg]_{f(\boldsymbol{x}_i)=f_{m-1}(\boldsymbol{x}_i)}.
\end{align}
Then we can update
\begin{align}
  \boldsymbol{f}_m = \boldsymbol{f}_{m-1} - \rho_m \boldsymbol{g}_m,
\end{align}
where $\rho_m$ is the step length and can be found by approximating the real function
\begin{align}
  h_m(\boldsymbol{x})= - \rho \boldsymbol{g}_m(\boldsymbol{x}).
\end{align}
So far, this only optimizes $f$ at a fixed set of points, but we can modify it by fitting a weak classifier to approximate the negative gradient. Additionally, we add a step length parameter $0<\nu<1$ to perform partial updates, also known as \textit{shrinking} \cite{Murphy2012}. The gradient boost algorithm is shown in Algorithm \ref{alg:gradientBoost}.

\begin{algorithm}[H]
\SetAlgoLined
  Initialize the estimate $f_0(\boldsymbol{x})$\;
 \For{$m = 1$ : $M$}{
  Compute the negative gradient vector $\boldsymbol{u}_m = - \partial C(\boldsymbol{y},\boldsymbol{f})/\partial \boldsymbol{f}(\boldsymbol{x})$ at $\boldsymbol{f}(\boldsymbol{x})=\boldsymbol{f}_{m-1}$\;
  Fit the base learner to the negative gradient $h_m(\boldsymbol{u}_m, \boldsymbol{x})$\;
  Update the estimate $f_m(\boldsymbol{x}) = f_{m-1} (\boldsymbol{x}) + \nu h_m (\boldsymbol{u}_m, \boldsymbol{x})$\;
 }
 Output the final estimation $f_M(\boldsymbol{x}) = \sum_{m=1}^M \nu h_m (\boldsymbol{u}_m, \boldsymbol{x})$
 \caption{Gradient boost algorithm.}
 \label{alg:gradientBoost}
\end{algorithm}

\section{Dimensionality reduction}
%There are several different methods to evaluate a model with many different types of scores such as accuracy, precision and F1-scores.
Supervised learning introduces models that can be easy to understand, visualize, and has well-defined tools and models. However, a dataset can be tedious to work with due to a large number of descriptors. These descriptors may also be correlated, which means that no new information will be learned from a correlated feature and therefore could be disregarded. Furthermore, a large dataset poses a computational challenge, and a reduction in descriptors could potentially reduce the computational time and effort required for any data analysis. Therefore, it would be beneficial to apply a method that finds correlated descriptors and reduce the dimensionality of a dataset. This is the idea of \textit{principal component analysis} (PCA).

%Unfortunately, this does not transfer to unsupervised learning. In unsupervised learning, there is no simple goal for the analysis and the evaluation tends to be of a subjective matter. Therefore, unsupervised learning is often used as an \textit{exploratory data analysis} \cite{James2017}. For data consisting of hundreds or thousands of features, it is possible to apply unsupervised learning to find correlated features and reduce dimensionality of the data, potentially reducing computational effort and time usage drastically. This is the idea of \textit{principal component analysis} (PCA).

\subsection{Principal component analysis}
\label{pca}

Principal component analysis is an algorithm that tries to find a low-dimension representation of a dataset that contains as much of the variance in the data as possible \cite{Murphy2012, James2017}. Each of the dimensions found by PCA are a linear combination of the features in the dataset, and are known as \textit{principal components}.

We can write the design matrix $\boldsymbol{X}\in {\mathbb{R}}^{n\times p}$, with $p$ features and $n$ entries, in terms of its column vectors as
\begin{align}
\boldsymbol{X}=\begin{bmatrix} \boldsymbol{x}_0 & \boldsymbol{x}_1 & \boldsymbol{x}_2 & \dots & \dots & \boldsymbol{x}_{p-1}\end{bmatrix},
\label{eq:designmatrix}
\end{align}
with a given vector
\begin{align}
\boldsymbol{x}_i^T = \begin{bmatrix}x_{0,i} & x_{1,i} & x_{2,i}& \dots & \dots x_{n-1,i}\end{bmatrix}.
\label{eq:pc}
\end{align}

\noindent Then we can compute the \textit{covariance matrix} of the design matrix $\boldsymbol{X}$, which is a measurement of the joint variability of the $p$ features in $\boldsymbol{X}$. The covariance is defined as
\begin{align}
\mathrm{cov}[\boldsymbol{v},\boldsymbol{u}] =\frac{1}{n} \sum_{i=0}^{n-1}(v_i- \overline{v})(u_i- \overline{u}),
\end{align}
where $\boldsymbol{v}$ and $\boldsymbol{u}$ are two vectors with $n$ elements each. The covariance matrix is defined by applying the covariance for every pairwise feauture, resulting in a $p\times p$ matrix. \begin{comment}On the diagonal, the covariance of two equal features becomes the variance of one,
\begin{align}
  \mathrm{cov}[\boldsymbol{u},\boldsymbol{u}] &=\frac{1}{n} \sum_{i=0}^{n-1}(u_i- \overline{u})(u_i- \overline{u}) = \mathrm{var}[\boldsymbol{u}].
\end{align}
The covariance can take values between $\pm \infty$, which give rise to computational issues due to loss of numerical precision. Therefore, we scale the covariance matrix by the variance. This is known as the correlation function
\begin{align}
  \mathrm{corr}[\boldsymbol{u},\boldsymbol{v}]=\frac{\mathrm{cov}[\boldsymbol{u},\boldsymbol{v}]}{\sqrt{\mathrm{var}[\boldsymbol{u}] \mathrm{var}[\boldsymbol{v}]}}.
\end{align}
Since all values are between $-1$ and $1$, we avoid any loss of numerical precision. The resulting covariance matrix $\boldsymbol{C} \in {\mathbb{R}}^{p\times p}$ becomes

\begin{align}
  \boldsymbol{C}[\boldsymbol{x}] = \begin{bmatrix}
\mathrm{var}[\boldsymbol{x}_0] & \mathrm{cov}[\boldsymbol{x}_0,\boldsymbol{x}_1]  & \mathrm{cov}[\boldsymbol{x}_0,\boldsymbol{x}_2] & \dots & \dots & \mathrm{cov}[\boldsymbol{x}_0,\boldsymbol{x}_{p-1}]\\
\mathrm{cov}[\boldsymbol{x}_1,\boldsymbol{x}_0] & \mathrm{var}[\boldsymbol{x}_1]  & \mathrm{cov}[\boldsymbol{x}_1,\boldsymbol{x}_2] & \dots & \dots & \mathrm{cov}[\boldsymbol{x}_1,\boldsymbol{x}_{p-1}]\\
\mathrm{cov}[\boldsymbol{x}_2,\boldsymbol{x}_0]   & \mathrm{cov}[\boldsymbol{x}_2,\boldsymbol{x}_1] & \mathrm{var}[\boldsymbol{x}_2] & \dots & \dots & \mathrm{cov}[\boldsymbol{x}_2,\boldsymbol{x}_{p-1}]\\
\dots & \dots & \dots & \dots & \dots & \dots \\
\dots & \dots & \dots & \dots & \dots & \dots \\
\mathrm{cov}[\boldsymbol{x}_{p-1},\boldsymbol{x}_0]   & \mathrm{cov}[\boldsymbol{x}_{p-1},\boldsymbol{x}_1] & \mathrm{cov}[\boldsymbol{x}_{p-1},\boldsymbol{x}_{2}]  & \dots & \dots  & \mathrm{var}[\boldsymbol{x}_{p-1}]\\
\end{bmatrix},
\end{align}
for all vectors $\boldsymbol{x}_i$ where $i=0,1,\dots,p-1$. The correlation matrix $\boldsymbol{K}[\boldsymbol{x}]$ becomes
\begin{align}
  \boldsymbol{K}[\boldsymbol{x}] = \begin{bmatrix}
  1 & \mathrm{corr}[\boldsymbol{x}_0,\boldsymbol{x}_1]  & \mathrm{corr}[\boldsymbol{x}_0,\boldsymbol{x}_2] & \dots & \dots & \mathrm{corr}[\boldsymbol{x}_0,\boldsymbol{x}_{p-1}]\\
  \mathrm{corr}[\boldsymbol{x}_1,\boldsymbol{x}_0] & 1  & \mathrm{corr}[\boldsymbol{x}_1,\boldsymbol{x}_2] & \dots & \dots & \mathrm{corr}[\boldsymbol{x}_1,\boldsymbol{x}_{p-1}]\\
  \mathrm{corr}[\boldsymbol{x}_2,\boldsymbol{x}_0]   & \mathrm{corr}[\boldsymbol{x}_2,\boldsymbol{x}_1] & 1 & \dots & \dots & \mathrm{corr}[\boldsymbol{x}_2,\boldsymbol{x}_{p-1}]\\
  \dots & \dots & \dots & \dots & \dots & \dots \\
  \dots & \dots & \dots & \dots & \dots & \dots \\
  \mathrm{corr}[\boldsymbol{x}_{p-1},\boldsymbol{x}_0]   & \mathrm{corr}[\boldsymbol{x}_{p-1},\boldsymbol{x}_1] & \mathrm{corr}[\boldsymbol{x}_{p-1},\boldsymbol{x}_{2}]  & \dots & \dots  & 1\\
  \end{bmatrix}.
\end{align}
The covariance matrix
\end{comment}
\noindent We can rewrite it as a function of the design matrix,
\begin{align}
  %\frac{1}{n}\boldsymbol{X}\boldsymbol{X}^T=
  \boldsymbol{C}[\boldsymbol{x}] = \mathbb{E}[\boldsymbol{X}\boldsymbol{X}^T] - \mathbb{E}[\boldsymbol{X}] \mathbb{E}[\boldsymbol{X}^T],
\end{align}
where $\mathbb{E}[\boldsymbol{X}\boldsymbol{X}^T]$ is the expectation value, and assuming we have normalized the data such that $\mathbb{E}[X] = 0$, we can remove the last term.

Further on, we assume that we can apply a number of orthogonal transformations by some orthogonal matrices $\boldsymbol{S}=[\boldsymbol{s}_0,\boldsymbol{s}_1,\dots,\boldsymbol{s}_{p-1}]\in {\mathbb{R}}^{p\times p}$ with the column vectors $\boldsymbol{s}_i \in {\mathbb{R}}^{p}$. Additionally, we assume that there is a transformation
\begin{align}
  \boldsymbol{C}[\boldsymbol{y}] =\boldsymbol{S}\boldsymbol{C}[\boldsymbol{x}]\boldsymbol{S}^T = \mathbb{E}[\boldsymbol{S}\boldsymbol{X}\boldsymbol{X}^T\boldsymbol{S}^T],
\end{align}
such that the new matrix $\boldsymbol{C}[\boldsymbol{y}]$ is diagonal with elements $[\lambda_0,\lambda_1,\lambda_2,\dots,\lambda_{p-1}]$. By multiplying with $\boldsymbol{S}^T$, we arrive at the given eigenvalue number $i$ of the covariance matrix that
\begin{align}
  \boldsymbol{s}^T_i\lambda_i = \boldsymbol{C}[\boldsymbol{x}]\boldsymbol{s}^T_i.
\end{align}

\noindent Dimensions with large eigenvalue have a large variation and can therefore be used to find features with useful information since we multiply the eigenvalue with the eigenvectors. When the eigenvalues are small, it means that the eigenvectors shrink accordingly and there is a small variation in these specific features \cite{Marsland2014}.

So far, we have been leading up to the classical PCA theorem. Assume that the data is represented as in \autoref{eq:designmatrix} with $\mathbb{E}[X]=0$, and assume that there exists an orthogonal transformation $\boldsymbol{W}\in {\mathbb{R}}^{p\times p}$. We can then define the reconstruction error

\begin{align}
  J(\boldsymbol{W},\boldsymbol{Z}) = \frac{1}{n}\sum_i (\boldsymbol{x}_i - \overline{\boldsymbol{x}}_i)^2,
\end{align}
with $\overline{\boldsymbol{x}}_i = \boldsymbol{W}\boldsymbol{z}_i$, where $\boldsymbol{z}_i$ is a column vector with dimension ${\mathbb{R}}^{n}$ of the matrix
$\boldsymbol{Z}\in{\mathbb{R}}^{p\times n}$. The PCA theorem states that minimizing the above reconstruction error corresponds to setting $\boldsymbol{W}=\boldsymbol{S}$, which is the orthogonal matrix that diagonalizes the covariance matrix \cite{Murphy2012}. The optimal number of features that correspond to the encoding is given by the set of vectors $\boldsymbol{z}_i$ with at most $l$ vectors. This is defined as the orthogonal projection of the data onto the columns spanned by the eigenvectors of the covariance matrix. Instead of using the covariance matrix, it is preferable to use the correlation matrix to avoid loss of numerical precision. Additionally, it is important to mention that the covariance matrix is sensitive to the standardization of variables, which is why one should always remember to center the data around before applying PCA.  We recommend the reader to read Ref. \cite{Murphy2012} p. $387$ for proof of the classical PCA theorem, as we will not elaborate any further. The algorithm for PCA is shown in Algorithm \ref{alg:pca}.

\begin{algorithm}[H]
\SetAlgoLined
 Set up the design matrix $\boldsymbol{X}\in {\mathbb{R}}^{n\times p}$ with $p$ features and $n$ entries;\\
 Center the data by subtracting the mean value for each column;\\
 Compute the covariance matrix $\mathbb{E}[\overline{\boldsymbol{X}}\overline{\boldsymbol{X}}^T]$; \\
 Find the eigenpairs of $\boldsymbol{C}$ with eigenvalues $[\lambda_0,\lambda_1,\dots,\lambda_{p-1}]$ and eigenvectors $[\boldsymbol{s}_0,\boldsymbol{s}_1,\dots,\boldsymbol{s}_{p-1}]$;\\
 Order the eigenvalues, and therefore also the eigenvectors, in descending order.
 Keep only those $l$ eigenvalues larger than a selected threshold value.
 \caption{Principal component analysis algorithm.}
 \label{alg:pca}
\end{algorithm}

\noindent Instead of choosing an arbitrary number of dimensions to reduce down to, it is common to choose the number of dimensions that accumulate a sufficient amount of variance. However, it remains a subjective analysis in how many principal components one should include as it will depend on both the specific application and specific data set. If it is impossible to give a motivation for reducing a large dataset to just two or three principal components, there might still be a reason why to apply PCA to a dataset. PCA can be applied as a preprocessing method to reduce the dimensionality of a dataset, and therefore might drastically improve the efficiency of further supervised learning approaches.

\section{Practical challenges associated with machine learning}

So far, we have covered substantially researched topics such as dimensionality reduction, supervised algorithms and metric evaluation. However, there exist parts of machine learning that do not necessarily get as much attention, but yet are crucial for the objective of machine learning. In this section, we will briefly mention both known and unknown challenges that are part of building a machine learning model.

The initial phase consists of gathering information systematically. This could be perhaps the most time-consuming part of the entire process, motivated by questions such as how much data is necessary. The answer to this question is as vague as the question itself, since there is no lower or upper bound but rather a general recommendation that the more data the better. Additionally, we should have a hypothesis that we are collecting descriptors of something that can explain the objective of the entire machine learning process. Indeed, the promises of machine learning are limited to data containing good descriptors. For a supervised learning algorithm, it is necessary to have one descriptor for the training data that contains information about what should be learned.

Thereafter follows an analysis of the data quality, often called \textit{pre-processing}. This includes identifying outliers and finding out what to do about any potential missing value in the data. Normally, solutions such as removing outliers and filling any missing value with either the mean, median or zero are applied. The data is also required to be transformed into continuous or categorical values. For the latter case, we can carry out a one-hot encoding to ensure that any algorithm does not assume one category being more important to another due to a larger number. Furthermore, it could be necessary to scale the data and reduce dimensionality, with the motivation discussed in \autoref{pca}.

If the algorithm has not been chosen yet, this is the time to do so. A clever first-hand approach is to apply a simple algorithm that is not computational demanding to see how it performs on a subset of the data. If the performance is satisfactory, any implementation of a more sophisticated algorithm could be redundant.

Next, the search for optimal hyperparameters while maintaining a generalized model can pose a challenge but is achievable. It is popular to apply cross-validation during this process with different evaluation metrics, as discussed in \autoref{evaluating accuracy}.

Eventually, with a chosen algorithm and its optimal hyperparameters, we train the algorithm on the entire preprocessed training data and then perform predictions on unseen data. To avoid any bias in the predictions, predictions on unseen data must be done only once. The reason for this is that we do not want to optimize any model for the actual test data, since this would reduce the generalization and increase the bias.

%is the data reliable??? bias? Quality of data? Are we measuring something that can explain the objective?


%- data gathering (long time. DO we have enough data??? As much data as possible)
%- data cleaning (Hows the quality? Remove outliers, fill missing values using mean, median or zero)
%- data labelling
%- choosing a statistical learning method
%- Apply preprocessing or not (continuous variables and categorical values)
%- find optimal hyperparameters while avoiding overfitting on subsets of data (computational effort)
%- have we achieved a generalized model? We can understand the simpler models, but as complexity increases we are bound to just accept many models as black boxes.
%- compute labels on unseen data (test data predictions)

%``Young technology is a double-edged sword. In one hand, it incorporates the latest technology and developments, but on the other hand, it is not production-ready''


%\section{Unsupervised learning}
%\subsection{Kmeans}
%\subsection{deep belief networks}
