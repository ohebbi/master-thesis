\chapter{Understanding information from ab-initio calculations}

In this work we will be looking into information gained by \textit{ab-initio} calculations, which means "from first principles". Initially, the method of the calculations can be regarded as a black box where one provides the structure of a material as input, which the black box in return feed us the outcome in terms of interesting constants or a different structure. In this chapter we provide the neccessary knowledge to understand what is happening inside the black box, and the quality of the output. The significance of this will be abundantly clear when we later have to interpret information regarding several thousands solid-state systems, and not a single material. In particular, the quality of the output is dependent on being consistent.

The black box is based on a successful theory called density functional theory (DFT), which is an approach for predicting physical properties of solid-state systems. We will ony summarize the neccessary theory behind density functional theory, leaving most of the quantum-mechanical world untouched. We will start with an explanation of why it is difficult to calculate interactions between particles, and then review key approximations and methods regarding the theory. However, even if density functional theory solves some problems, it also introduce new challenges, which will be thoroughly discussed.

%Lastly, we will in particular look into

%canonical variables
%dynamical variables
%operator
%canonical substitusion

\begin{comment}
\section{The single-electron Schrödringer equation}

We will start of investigating the Schrödinger equation with only one electron \cite{Griffiths2017}
\begin{align}
    i\hslash \frac{\partial \Psi}{\partial t} = -\frac{\hslash^2}{2m}\nabla^2 \Psi + V\Psi
    \label{eq:Schrödinger}
\end{align}
for a convenient external potential $V_{ext}(r)$ that is independent of time. We will try to look for solutions for (\ref{eq:Schrödinger}) by separating the wave function into a space-dependent and time-dependent function
%Most wavefunctions are solutions to (\ref{eq:Schrödinger}), but if the wavefunction describes a stationary state, the wavefunction has to be an eigenfunction to H for reasons that will become clear shortly.
\begin{align}
  \Psi(r,t) = \psi(r)\phi(t).
  \label{eq:separation}
\end{align}
By inserting ordinary derivatives and dividing each side with equation (\ref{eq:separation}), our Schrödinger equation (\ref{eq:Schrödinger}) now reads
\begin{align}
  i\hslash \frac{1}{\phi(t)}\frac{d\phi(t)}{dt} = - \frac{\hslash^2}{2m} \frac{1}{\psi(r)}\nabla^2 \psi + V(r)
\end{align}

Since the potential function $V(r)$ is independent of time, we observe the time and space dependencies of each side and state the fact that both sides has to be constant. Thus, two intriguing equations unveil themselves;
%captivating?
\begin{align}
  i\hslash \frac{1}{\phi(t)}\frac{d\phi(t)}{dt} = E\phi(t)
  \label{eq:time}
\end{align}
and
\begin{align}
  \frac{\hslash^2}{2m} \frac{1}{\psi(r)}\nabla^2 \psi + V(r) = E\psi(r)
  \label{eq:tise}
\end{align}
where the first equation (\ref{eq:time}) has a general solution $\phi(t) = C \exp (-iEt/\hslash)$ and $C=1$ after normalization, and the second equation (\ref{eq:tise}) is known as time-independent Schrödinger equation. These two equations are connected through the variable $\varepsilon$.

By utilizing variable separation to get equation (\ref{eq:separation}), we find that the wavefunction is describing a stationary state with probability density
\begin{align*}
  \lvert \Psi (r,t)\rvert ^2 &= \Psi^*\Psi \\
  &= \Psi^* e^{iEt/\hslash} \Psi e^{-i Et/\hslash} \\
  &= \lvert \Psi (r)\rvert ^2
\end{align*}
that is independent of time. Conveniently, this is also true for every expectation value; they are all constant in time. We can also try to express this in classical terms regarding the Hamiltonian, which in this scenario is defined as
\begin{align}
    \hat{H}(r, p) = \frac{p^2}{2m} + V(r) = -\frac{\hslash^2}{2m}\nabla^2 + V(r)
\end{align}
simplifying equation \ref{eq:tise} to
\begin{align}
  \hat{H} \psi = E\psi
  \label{eq:tise_nesten}
\end{align}
and we can find the expectation value of the total energy as
\begin{align*}
    \langle H \rangle &= \int \psi^* \hat{H} \psi dr \\
                &= E\int \lvert \Psi \rvert ^2 dr \\
                &= E
\end{align*}
using the fact that expectation values are constant in time for stationary states. Similarly, we can try to estimate the variance of the Hamiltonian,
\begin{align*}
  \sigma_H^2 &= \langle H^2 \rangle - \langle H \rangle ^2 \\
            &= E^2 - E^2 \\
            &= 0
\end{align*}
which appropiately describes that every measurement of the total energy is certain to return the value E.

\subsection{Eigenfunctions}
So far, we have not given an explanation of what a wavefunction is. As a matter of fact, we have actually found an eigenfunction
\begin{align*}
  \psi_\kappa(r,t) = \psi_\kappa e^{-i\varepsilon_\kappa t/\hslash}
\end{align*}
where $\kappa$ denotes the $k$-th eigenfunction and $\varepsilon_\kappa$ is its corresponding energy eigenvalue. The eigenfunctions have distinct energies and have the attribute that they are orthogonal and normalized with respect to
\begin{align*}
  \bra{\psi_\kappa (r,t)} \ket{\psi_{\kappa`} (r,t)} = \delta_{\kappa \kappa'}.
\end{align*}
The state with the lowest energy is called the ground state, and is where it is most likely to find an electron in a single-electron system with no external potential applied.

A general wavefunction can be generated by a summation of eigenfunctions (such as the eigenfunction in the latter case)
\begin{align}
\Psi(r,t) = \sum_\kappa c_\kappa \psi_{\kappa}(r,t),
\end{align}
where $c_\kappa$ is a constant. A general wavefunction does not neccessarily describe stationary states, and consequently does not have distinct energies but is rather represented statistically from the expectation value
\begin{align*}
  E = \sum_{\kappa} \lvert c_\kappa \rvert \varepsilon_\kappa.
\end{align*}
Solving Schrödinger equation for a general wavefunction is rather troublesome. Fortunately, we can use the eigenfunctions instead, transforming equation \ref{eq:tise_nesten} into time-independent Schrödinger equation for eigenfunctions
\begin{align}
  \hat{H} \psi_{\kappa}(r) = \varepsilon_\kappa \psi_\kappa(r).
\end{align}

The shape of en eigenfunction has normally high spatial symmetri that depends on the symmetri of the potential $V_{ext}(r)$ and the boundary conditions \cite{Persson2020}. The study of how atoms in a crystalline interact with each other is of upmost importance when trying to explain macroscopic consequences.

\end{comment}
\section{Introduction to density functional theory}

To fully understanding what challenges the density functional theory solves, we will need to investigate how we can calculate the forces acting inside a crystal. Since these forces are happening on a microscopic scale, we will need to utilize the theory of quantum mechanics.


%However, the fundamental theory remains the same and we will start our venture with the Schrödinger equation.

\subsection{The Schrödinger equation}

In principle, we can describe all physical phenomenas of a system with the wavefunction $\Psi(\textbf{r},t)$ and the Hamiltonian $\hat{H}(\textbf{r},t)$, where $\textbf{r}$ is the spatial position and $t$ is the time. Unfortunately, analytical solutions for the the time-dependent Schrödinger equation,
\begin{align}
    i\hslash \frac{\partial}{\partial t} \Psi(\textbf{r},t) = \hat{H}(\textbf{r},t) \Psi(\textbf{r},t),
    \label{eq:tdse}
\end{align}
are extremely rare. More conveniently, we can generate a general wavefunction by a summation of eigenfunctions,
\begin{align}
  \Psi(\textbf{r},t) = \sum_\kappa c_\kappa \psi_\kappa(\textbf{r},t),
\end{align}
where $c_\kappa$ is a constant and $\psi_\kappa$ is the $\kappa$-th eigenfunction. A general wavefunction does not neccessarily describe stationary states, and consequently does not have distrinct energies but is rather represented statistically from the expectation value
\begin{align}
  E = \sum_\kappa \lvert c_\kappa \rvert E_\kappa.
\end{align}

 Solving the Schrödinger equation for a general wavefunction is rather troublesome, but luckily we can use the eigenfunctions instead, transforming equation \ref{eq:tdse} into the time-independent Schrödinger equation for eigenfunctions
\begin{align}
  \hat{H}\psi_\kappa(\textbf{r}) = E_\kappa \psi_k(\textbf{r}),
\end{align}
where $E_\kappa$ is the eigenvalue of the $\kappa$-th eigenstate $\psi_\kappa(\textbf{r})$. The eigenfunctions have distinct energies, and the state with the lowest energy is called the ground state. They have the attribute that they are orthogonal and normalized with respect to
\begin{align}
  \left \langle \psi_\kappa \left(\textbf{r}\right) \rvert \psi_{\kappa`} \left(\textbf{r} \right) \right \rangle = \delta_{\kappa \kappa'}.
\end{align}
The symmetry of an eigenfunction depends on the symmetry of the potential $V_{ext}(\textbf{r})$ and the boundary conditions \cite{Persson2020}.

\subsection{The many-particle Schrödinger equation}
As we extend the theory to include many-particle systems, we will gradually explain and add the different contributions that make up the many-body Hamiltonian. During this process, we will neglect any external potential applied to the system.

If we place a simple electron with mass $m_e$ in its own system, it will be in  possession of kinetic energy. Instead of just one electron, we can place $N_e$ electrons, and they will together have the total kinetic energy
\begin{align}
  T_e = - \sum_{j=1}^{N_e} \frac{\hslash^2\nabla_j}{2m_e}.
\end{align}
All the electrons are negatively charged, causing repulsive Coulomb interactions between each and every electron, totalling to
\begin{align}
  U_{ee} = \sum_{j=1}^{N_e}\sum_{j'<j} \frac{q^2}{\lvert r_j - r_{j'}\rvert}.
  \label{eq:electron-electron}
\end{align}
The summation voids counting each interaction more than once. Simultaneously, we can place $N_n$ nuclei with mass $m_n$ in the same system, accumulating the kinetic energy
\begin{align}
  T_n = - \sum_{a=1}^{N_n} \frac{\hslash^2\nabla_a}{2m_n}.
\end{align}
As in the example with electrons, the nuclei are also experiencing repulsive interactions between every single nucleus, adding up the total interactions as
\begin{align}
  U_{nn} = \sum_{a=1}^{N_n}\sum_{a'<a} \frac{q^2 Z_aZ_{a'}}{\lvert R_a - R_{a'}\rvert }.
\end{align}
where $Z_a$ is the atom number of nuclei number $a$.

The system now contains $N_e$ electrons and $N_n$ nuclei, thus we need to include the attractive interactions between the them,
\begin{align}
  U_{en} = - \sum_{j=1}^{N_e} \sum_{a=1}^{N_n} \frac{q^2Z_a}{\lvert r_j-R_a\rvert}.
\end{align}

Together, these equations comprise the time-independent many-particle Hamiltonian
\begin{align}
  \begin{aligned}
    \hat{H} = &- \sum_{j=1}^{N_e} \frac{\hslash^2\nabla_j}{2m_e} - \sum_{a=1}^{N_n} \frac{\hslash^2\nabla_a}{2m_n} + \sum_{j=1}^{N_e}\sum_{j'<j} \frac{q^2}{\lvert r_j - r_{j'}\rvert} \\ &+\sum_{a=1}^{N_n}\sum_{a'<a} \frac{q^2 Z_aZ_{a'}}{\lvert R_a - R_{a'}\rvert } - \sum_{j=1}^{N_e} \sum_{a=1}^{N_n} \frac{q^2Z_a}{\lvert r_j-R_a\rvert}.
  \end{aligned}
\end{align}


A few problems arise when trying to solve the many-particle Schrödinger equation. Firstly, the amount of atoms in a crystal is very, very massive. As an example, we can numerically try to calculate the equation \ref{eq:electron-electron} for a $1$mm$^3$ silicon-crystal that contains $7\cdot 10^{20}$ electrons. For this particular problem, we will pretend to use the current fastest supercomputer Fugaku \cite{Top500} that can calculate $514$ TFlops, and we will assume that we need $2000$ Flops to calculate each term inside the sum \cite{Persson2020}, and we need to calculate it $N_e \cdot N_e/2$ times for the (tiny) crystal. The entire electron-electron interaction calculation would take $2.46 \cdot 10^{19}$ years to finish for a tiny crystal. Thus, the large amount of particles translates into a challenging numerical problem.

%In one cubic-centimeter of a crystal, there are around $10^{23}$ electrons. This number is roughly the same as the number of stars in the universe, grain of sand on all beaches in the world, or currently $1.41\cdot 10^{19}$ times the amount of Home and Away episodes made since 1988.

Secondly, the many-particle Hamiltonian contains operators that has to be applied to single-particle wavefunctions, and we have no prior knowledge of how $\Psi$ depends on the single-particle wavefunctions $\psi_\kappa$.


\subsection{The Born-Oppenheimer approximation}
The many-particle eigenfunction describes the wavefunction of all the electrons and nuclei and we denote it as $\Psi_{\kappa}^{en}$ for electrons (e) and nuclei (n), respectively. The Born-oppenheimer approximation assumes that nuclei, of substantially larger mass than electrons, can be treated as fixed point charges. According to this assumption, we can separate the eigenfunction into an electronic part and a nuclear part,
\begin{align}
  \Psi_\kappa^{en}(\textbf{r}, \textbf{R}) \approx \Psi_{\kappa}(\textbf{r}, \textbf{R})\Theta_{\kappa}(\textbf{R}),
\end{align}
where the electronic part is dependent on the nuclei. This is in accordance with the assumption above, since electrons can respond instantaneously to a new position of the much slower nucleus, but this is not true for the opposite scenario. From here, one can obtain the electronic and nuclear eigenfunction, with the derivation shown in \ref{appendix:Born-Oppenheimer}.


\subsection{The Hartree and Hartree-Fock approximation}
\begin{comment}
As we venture along from a one-electron system to a two-electron systen, we encounter a new wavefunction and Hamiltonian that needs to describe two particles, making the two-electron Schrödinger equation read

\begin{align}
  \Big( -\frac{\hslash^2 \nabla_1^2}{2m_e} - \frac{\hslash^2\nabla_2^2}{2m_e}+ \frac{q^2}{\lvert r_1-r_2  \rvert} + V_{ext}(r) \Big) \Psi_\kappa (r_1, r_2) = E_{\kappa} \Psi_\kappa (r_1, r_2),
\end{align}
where the two first terms are the kinetic energies of the electrons, while the third term is a potential that describes the repulsive Coloumb interaction between the two electrons. The last term is the external potential, well known from the earlier scenario with only one electron.
\end{comment}
The next question in line is to find a wavefunction $\Psi(\textbf{r},\textbf{R})$ that depends on all of the electrons in the system. The Hartre \cite{Persson2020} approximation to this is to assume that electrons can be described independently, suggesting the \textit{ansatz} for a two-electron wavefunction
\begin{align}
  \Psi_\kappa(\textbf{r}_1,\textbf{r}_2) = A \cdot \psi_1(\textbf{r}_1) \psi_2(\textbf{r}_2),
\end{align}
where $A$ is a normalization constant. This approximation simplifies the many-particle Shrödinger equation a lot, but comes with the downside that the particles are distinguishable and do not obey the Pauli exclusion principle for fermions.

The Hartree-fock approach, however, overcame this challenge and presented an anti-symmetric wavefunction that made the electrons indistinguishable \cite{Griffiths2017}:
\begin{align}
  \Psi_\kappa(\textbf{r}_1,\textbf{r}_2) = \frac{1}{\sqrt{2}}\Big( \psi_1(\textbf{r}_1) \psi_2(\textbf{r}_2)  - {\psi_1(\textbf{r}_2)\psi_2(\textbf{r}_1)}\Big).
\end{align}
For systems containing more than one particles, the factor $1/\sqrt{2}$ becomes the Slater determinant and is used to normalize the wave function.

\subsection{The variational principle}
So far, we have tried to make the time-independent Schrödinger equation easier with the use of an \textit{ansatz}, but we do not neccessarily have an adequate guess for the eigenfunctions and the ansatz can only give a rough estimate in most scenarios. Another approach, namely the \textit{variational principle}, states that the energy of any trial wavefunction is always an upper bound to the exact ground state energy by definition $E_0$.
\begin{align}
  E_0 = \bra{\psi_0 } H \ket{\psi_0} \leq \bra{\psi}H\ket{\psi} = E
\end{align}

This enables a minimization of energy in terms of wavefunction parameters. A more thorough walk-through of the variational principle is included in appendix \ref{appendix:variational-principle}.

\section{The density functional theory}

Hitherto we have tried to solve the Schrödinger equation to get a ground state wave function, and from there we can obtain ground state properties, such as the ground state total energy. One fundamental problem that exists when trying to solve the many-electron Schrödinger equation is that the wavefunction is a complicated function that depends on $3N_e$ variables\footnote{not including spin}.

Hohenberg and Kohn \cite{Hohenberg1964} showed in 1964 that the ground-state density $n_0(r) = \lvert \Psi_0 (r)\rvert$ determines a general external potential, which includes $U_{en}$, up to an additive constant, and thus also the Hamiltonian \cite{Toulouse2019}. From another point of view, the theory states that all physical ground-state properties of the many-electron system are unique functionals of the density \cite{Persson2020}. A consequence of this is that the number of variables is reduced from $3N_e$ to $3$, significantly reducing the computational efforts.

However, the scheme is not without limitations, as the density functional theory (DFT) can only be used to find all the ground-state physical properties if the exact functional of the electron density is known. And $57$ years after Hohenberg and Kohn published their paper, the exact functional still remains unknown.

We will start this chapter with a brief mention of the Hohenberg-Kohn theorems and its implications, before we delve further into the Kohn-Sham equation. From the

\subsection{The Hohenberg-Kohn theorems}

\begin{theorem}
  For any system of interacting particles in an external potential $V_{ext}$, the density is uniquely determined.
\end{theorem}

The theorem can be proved by utilising the variational principle for two different external potentials with the same ground state density.
The proof is included in appendix \ref{appendix:theorem1}.

\begin{theorem}
  There exists a variational principle for the energy density functional such that, if $n$ is not the electron density of the ground state, then $E\left[ n_0 \right] < E\left[ n \right]$.
\end{theorem}

From theorem 1, we know that the external potential is uniquely determined by the density, which in turn uniquely determines the ground state wavefunction. Therefore, all other observables of the system are uniquely determined and we can express the energy as function of the density,
\begin{align}
  E[n] = \overbrace{T[n] + U_{ee}[n]}^{F[n]} + U_{en}[n].
  \label{eq:densityfunctional}
\end{align}
where $F[n]$ is an universel functional known as the Hohenberg-Kohn functional. The proof for theorem 2 is found in appendix \ref{appendix:theorem2}.

\subsection{The Kohn-Sham equation}
So far, we have tried to make the challenging Schrödinger equation less challenging by simplifying it, with the last attempt containing the Hohenberg-Kohn's theorems where the theory states that the total ground-state energy can, in principle, be determined exactly once we have found the ground-state density.

In 1965, Kohn and Sham \cite{Kohn1965} reformulated the Hohenberg-Kohn theorems by generating the exact ground-state density $n_0(r)$ using a Hartree-like total wavefunction
\begin{align}
    \Psi(\textbf{r}_1,\textbf{r}_2,..,\textbf{r}_{N_e}) = \psi_1^{KS}(\textbf{r}_2)\psi_2^{KS}(\textbf{r}_2)...\psi_{N_e}^{KS}(\textbf{r}_{N_e}),
\end{align}
where $\psi_j^{KS}(r_j)$ are some auxiliary independent single-particle wavefunctions. However, the Kohn-Sham wavefunctions cannot be the correct single-particle wavefunctions since our ansatz implies an exact density
\begin{align}
  n(\textbf{r}) = \sum_{j=1}^{N_e}\lvert \psi_j^{KS}(\textbf{r})\rvert^2.
\end{align}
Recalling that equation \ref{eq:densityfunctional} describes the total energy as a functional of the density,
\begin{align}
  E[n] = T[n] + U_{ee}[n] + U_{en}[n],
\end{align}
we try to modify it to include the kinetic energy $T_s[n]$ and the interaction energy $U_s[n]$ of the auxiliary wavefunction, and the denotation $s$ for single-particle wavefunctions.
\begin{align*}
  E[n] &= T[n] + U_{ee}[n] + U_{en}[n] + \left( T_s[n] - T_s[n] \right) + \left( U_s[n] - U_s[n] \right) \\
  &= T_s[n] + U_{s}[n] + U_{en}[n] + \underbrace{\left(T[n] - T_s[n] \right) + \left( U_{ee}[n] - U_s[n] \right)}_{E_{xc}[n]}
\end{align*}
Here we have our first encounter with the \textit{exchange-correlation energy}
\begin{align}
  E_{xc}[n] = \Delta T + \Delta U = \left(T[n] - T_s[n] \right) + \left( U_{ee}[n] - U_s[n] \right),
\end{align}
which contains the complex many-electron interaction. For non-interacting system, $E_{xc}[n]$ is conveniently zero, but in interacting systems it most likely is a complex expression. However, one can consider it as our mission to find good approximations to this term, as the better approximations, the closer we get to the exact expression.

The exact total energy functional can now be expressed as
\begin{align}
  \begin{aligned}
  E[n]
  &= \overbrace{\sum_j \int \psi_j^{KS*} \frac{-\hslash^2\nabla^2}{2m} \psi_j^{KS}d\textbf{r}}^{T_s[n]} + \overbrace{\frac{1}{2}\int \int q^2\frac{n(\textbf{r})n(\textbf{r}')}{\lvert \textbf{r}-\textbf{r}'\rvert} d\textbf{r}d\textbf{r}'}^{U_s[n]}
  \\ &+ \underbrace{\int V_{en}(\textbf{r})n(\textbf{r})d\textbf{r}}_{U_{en}[n]} + \underbrace{\left(T[n] - T_s[n] \right) + \left( U_{ee}[n] - U_s[n] \right)}_{E_{xc[n]}}.
  \end{aligned}
\end{align}
given that the exchange-correlation functional is described correctly. By utilizing the variational principle, we can now formulate a set of Kohn-Sham single-electron equations,
\begin{align}
  \left\{ -\frac{\hslash^2}{2m_e}\nabla^2_s + V_H(\textbf{r}) + V_{j\alpha}(\textbf{r}) + V_{xc}(\textbf{r}) \right\} \psi_s^{KS}(\textbf{r}) = \epsilon_s^{KS} \psi_s^{KS}(\textbf{r})
  \label{eq:singleKS}
\end{align}
where $V_{xc}(\textbf{r})=\partial E_{xc}[n]/\partial n(\textbf{r})$ and $V_{H}(\textbf{r})=\int q^2 \frac{n(\textbf{r'})}{\lvert \textbf{r} - \textbf{r}'\rvert} d\textbf{r}'$ is the Hartree potential describing the electron-electron interaction. It is worth to notice that $V_H(\textbf{r})$ allows an electron to interacts with itself, resulting in a self-interaction contribution, however this will be taken care of in $V_{xc}$.

Finally, we can define the total energy of the system according to Kohn-Sham theory as
\begin{align}
  E[n] = \sum_{j}\epsilon_j^{KS}-\frac{1}{2}\int \int q^2 \frac{n(\textbf{r})n(\textbf{r}')}{\lvert \textbf{r} - \textbf{r}' \rvert} d\textbf{r}d\textbf{r}' + E_{xc}[n] - \int V_{xc}(\textbf{r})n(\textbf{r})d\textbf{r}.
\end{align}
If $V_{xc}$ is exact, and $E[n]$ gives the true total energy, we still do not know if the energy eigenvalues $\epsilon_s^{KS}$ are the true single-electron eigenvalues. However, there exists one exception, which is that the highest occupied eigenvalue of a finite system has to be exact if the density is exact.

The only task that is left for us now is to find the exact expression for $E_{xc}[n]$ as a functional of the density $n(r)$. With that expression, we would be able to calculate the total energies of any material, and most likely solve a few of the biggest puzzles in the history of humankind. Unfortunately, the exchange-correlation potential is unknown for most systems.

It is possible to solve the Kohn-Sham equations by applying a self-consistent field method. This is a computational scheme, and for further details one can consult the appendix \ref{appendix:self-consistent}.

%In this approximation we have used Hartree energy where the self-interaction correction is neglected, raising less accurate results. It is possible to use other approximations for $T_s[n]$ and $U_s[n]$ than Hartree wavefunctions, and good approximations will result in a low $E_{xc}[n]$ because $T_s[n]$ and $U_s[n]$ will get closer and closer to the true values.


%The next step in finding the KS-equation is to utilize the variational principle in the purpose of finding the ground-state energy. First one minimises the total energy with respect to each of the wavefunctions with the constraint that the wavefunctions should be orthonormalized;
%\begin{align*}
%  \frac{\partial }{\partial \psi_j^{*} (\textbf{r})}E[n] &= \sum_{i,j}\lambda_{ij}\int \psi_i^{s*}(\textbf{r}_i)\psi_j^{s}(\textbf{r}_j)d\textbf{r}d\textbf{r}_j\\
%  \frac{\partial }{\partial \psi_j^{*} (\textbf{r})}E[n] &= \lambda_j\psi_j^s (\textbf{r}_j)
%\end{align*}

\subsection{The exchange-correlation energy}


There is one scenario for which we can derive the exact expression of the exchange-correlation functional, namely the \textit{homogeneous electron gas} (HEG). However, this has a natural cause, since by definition $n(\textbf{r})$ is constant for this situation. Given that it is the variations of electron density that are the foundation of material properties, the usefulness of HEG is limited. The \textit{local density approximation} (LDA) is an approximation based on this approach, where the local density is the only variable used to define the exchange-correlation functional. Specifically, we can set the exchange-correlation potential at each position to be the known exchange-correlation potential from homogeneous electron gas at the electron density observed at that position \cite{Kohn1965}:
\begin{align}
  V_{xc}(\textbf{r}) = V_{xc}^{\text{electron gas }}\left[n(\textbf{r})\right].
\end{align}
This is the simplest and most known approximation to the exchange-correlation functional, and accordingly it has a few drawbacks. One of them is the incomplete cancellation of the self-interaction term, which leads to a repulsion that may cause artifical repulsion between electrons, and hence increased electron delocalization \cite{Allen2014}. In addition, LDA has proven challenging  to use when studying atoms and molecules because of their rapidly varying electron densities, however, the LDA is seen as succesful for bulk materials because of the slowly varying electron density \cite{DavidSholl2009}. Considering the relatively low computational cost and relatively high accuracy, the LDA overall makes a good model for estimation of the exchange-correlation functional for bulk-materials.

In the light of the merits of the LDA, an extensive search for new approximations was launched. The \textit{generalized gradient approximation} (GGA) is an extension of the LDA, which includes the gradient of the density
\begin{align}
  V_{xc}^{GGA}(\textbf{r}) = V_{xc}\left[ n(\textbf{r}), \nabla n (\textbf{r})\right].
\end{align}
The GGA is a good approximation for the cases where the electron density varies slowly, but faces difficulties in many materials with rapidly varying gradients in the density, causing the GGA to fail. Thus, the annotation \textit{generalized} in GGA is set to include the different approaches to deal with this challenge. Two of the most commonly implemented GGA functionals are the non-empirical approaches Perdew-Wang 91 (PW91) \cite{Perdew1992} and Perdew-Burke-Ernzerhof (PBE) \cite{Perdew1996}.

Both LDA and GGA are commonly known to severely underestimate the band gaps of semiconductor materials, in addition to incorrectly predicting charge localizations originating from narrow bands or associated with local lattice distortions around defects \cite{Freysoldt2014}. The latter limitation is thought to be due to self-interaction in the Hartree potential in equation \ref{eq:singleKS}.

To improve the accuracy of excited state properties estimations, other methods have been developed. Tran and Blaha (TB) \cite{Tran2009} adapted the exchange potential suggested by Becke-Roussel (BR) \cite{Becke2006} that leads to band gaps close to experimental values while still using a cheap semilocal method \cite{Koller2011}. The modified TB-mBJ version introduces a parameter relative to the BJ version,
\begin{align}
    V_{xc}^{TB-mBJ}(\textbf{r}) =  c\cdot V_{xc}^{BR}(\textbf{r}) + (3c-2) \frac{1}{\pi} \sqrt{\frac{5}{6}} \sqrt{\frac{t(\textbf{r})}{\rho(\textbf{r})}},
\end{align}
where $\rho$ is the electron density, $t$ is the kinetic-energy density, $V_{xc}^{BR}$ is the Becke-Roussel exchange potential and $c$ is the parameter that changes weights of the BJ potential and is fit to experimental data.

Hybrid functionals intermix exact Hartree-Fock exchange with exchange and correlation from functionals based on the LDA or GGA. Hartree-Fock theory completely ignore correlation effects, but account for self-interaction and treats exchange as exact. Since LDA/GGA and Hartree-Fock supplement each other, they can be used as a combination for hybrid-functionals resulting in some cancellation of the self-interaction error. Becke \cite{Becke1993} introduced a $50\%$ Hartree-Fock exact exchange and
$50\%$ LDA energy functional, while Perdew \textit{et al.} \cite{Perdew1996a} altered it to $25\%-75\%$ and favoring PBE-GGA instead of LDA.

The inclusion of Hartree Fock exchange improves the description of localized states, but requires significantly more computational power for large systems. Another method called the \textit{GW} approximation includes screening of the exchange interaction \cite{Aryasetiawan1998}, but has a computational price that does not neccessarily defend its use. Thus, the real challenge is to reduce the computational effort while still producing satisfactory results. Heyd \textit{et al.} \cite{Heyd2003} suggested to separate the non-local Hartree-Fock exchange into a short- and long-range portion, incorporating the exact exchange in the short-range contribution. The separation is controlled by an adjustable parameter $\omega$, which was empirically optimised for molecules to $\omega = 0.15$ and solids to $\omega = 0.11$ and are known as the HSE03 and HSE06 (Heyd-Scuseria-Ernzerhof), respectively \cite{Krukau2006}. The functionals are expressed as
\begin{align}
  E_{xc}^{HSE} = aE_{x}^{\text{HF,SR}}(\omega) + (1-a)E_x^{\text{PBE,SR}}(\omega) + E_x^{\text{PBE,LR}}(\omega) + E_c^{\text{PBE}}
\end{align}
where $a=1/4$ is the Hartree-Fock mixing constant and the abbreviations SR and LR stands for short range and long range, respectively.

Hence, hybrid-functionals are \textit{semi-empirical} functionals that rely on experimental data for accurate results. They give accurate results for several properties, such as energetics, bandgaps and lattice parameters, and can fine-tune parameters fitted to experimental data for even higher accuracy.

Furthermore, the computational effort required for the hybrid-functionals are significantly larger than for non-empirical functionals such as LDA or GGA. Krukau \textit{et al.} \cite{Krukau2006} reported a substantial increase in computational cost when reducing the parameter $\omega$ from $0.20$ to $0.11$ for 25 solids, and going lower than $0.11$ demanded too much  to actually defend its use.

Unfortunately, an area where both GGA and hybrid functionals are reportedly inadequate is in calculating the dispersion interactions \cite{Klimes2009}. Many implementations have been developed to deal with this vulnerability, with one of them being the non-local van der Waals density functional (vdW-DF) \cite{Dion2004}
\begin{align}
  E_{xc} = E_{x}^{GGA} + E_{c}^{\text{LDA}} + E_{c}^{nl},
\end{align}
where $E_{x}^{GGA}$ is GGA exchange energy \cite{Klimes2009}.

\subsection{Limitations of the DFT}

If we had known the exact exchange-correlation functional, the density functional theory would yield the exact total energy. Alas, that is not the case and we are bound to use approximations in forms of functionals. What is common for all approximations is that they are specifically designed to solve one given optimization, therefore it is not neccessarily one functional that is consider superior in all fields of interest. One could consider that the hybrid functionals due to a high accuracy overall should be dominant, but that is only if one have the computational capacity required. The accuracy of calculations is dependent on which functional being used, and normally a higher accuracy means the use of a more complex and computationally demanding computational functional.

Nonetheless, density functional theory is considered a very successful approach and Walter Kohn was awarded the Nobel Price in chemistry in 1998 for his development of the density-functional theory \cite{Freitas1999}. It is especially regarded as successful in contexts where DFT can make important contributions to scientific questions where it would be essentially impossible to determine through experiments \cite{DavidSholl2009}. One can only hope that the future will be as bright as the past, and that this successful theory provides incentives for further growth in the next generation.


%\section{Input and output of ab-initio methods}
